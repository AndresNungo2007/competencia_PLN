{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCqTakHVKpnO"
      },
      "source": [
        "![Universidad_de_los_Andes_30.png](data:image/png;base64,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)\n",
        "\n",
        "# Open Evaluation - Consultas Abiertas en Lenguaje Natural\n",
        "\n",
        "## Descripci√≥n del Proyecto\n",
        "\n",
        "Este notebook demuestra la capacidad del modelo entrenado para procesar **consultas abiertas** en lenguaje natural y generar √≥rdenes de compra en formato JSON estructurado.\n",
        "\n",
        "**Objetivo**: Validar que el modelo puede generalizar a textos no vistos durante el entrenamiento y generar JSONs v√°lidos con estructura consistente seg√∫n el esquema de la competencia.\n",
        "\n",
        "**Contexto de negocio**: Una compa√±√≠a de ventas recibe pedidos por m√∫ltiples canales (email, SMS, Markdown). Este modelo automatiza la conversi√≥n a un formato est√°ndar JSON para procesamiento posterior.\n",
        "\n",
        "---\n",
        "\n",
        "## Integrantes Equipo 20\n",
        "- Andr√©s Felipe √ëungo Fern√°ndez  \n",
        "- Andr√©s Juli√°n Gonzalez Barrera  \n",
        "- Hernando Jose Jimenez D√≠az  \n",
        "- Gloria In√©s L√≥pez Urbano\n",
        "\n",
        "---\n",
        "\n",
        "## Requisitos del Proyecto\n",
        "\n",
        "Este notebook es uno de los **4 entregables obligatorios** del proyecto final:\n",
        "\n",
        "| # | Archivo | Descripci√≥n |\n",
        "|---|---------|-------------|\n",
        "| 1 | `train.ipynb` | Entrenamiento del modelo ||\n",
        "| 2 | `weights.pt` | Pesos entrenados (generado por train)  |\n",
        "| 3 | `evaluation.ipynb` | Evaluaci√≥n en test set |  |\n",
        "| 4 | **`open-evaluation.ipynb`** | **Este notebook** - Consultas abiertas  |\n",
        "\n",
        "---\n",
        "\n",
        "## Estructura del Notebook\n",
        "\n",
        "1. **Configuraci√≥n Inicial**: Librer√≠as, dispositivo, y par√°metros\n",
        "2. **Carga del Modelo**: Tokenizer + Modelo base + Adaptadores LoRA\n",
        "3. **Funci√≥n Principal**: `generar_orden_compra()`\n",
        "4. **Ejemplos de Uso**: 4+ casos variados (b√°sico, completo, incompleto, multiproducto)\n",
        "5. **Consulta Interactiva**: Para probar tus propios textos\n",
        "6. **An√°lisis de Resultados**: Verificaci√≥n de estructura JSON\n",
        "7. **Exportaci√≥n**: Guardar resultados\n",
        "\n",
        "---\n",
        "\n",
        "## Esquema JSON Esperado\n",
        "\n",
        "El modelo genera JSONs con la siguiente estructura:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"buyer\": {\n",
        "    \"name\": \"string\",\n",
        "    \"email\": \"string\",\n",
        "    \"contact\": {\"phone\": \"string\", \"preferred_contact\": \"email|phone|none\"},\n",
        "    \"addresses\": [{\"street\": \"...\", \"city\": \"...\", \"country\": \"...\"}]\n",
        "  },\n",
        "  \"purchases\": [\n",
        "    {\"product_name\": \"string\", \"quantity\": integer, \"currency\": \"USD|EUR|GBP\"}\n",
        "  ],\n",
        "  \"shipping\": {\n",
        "    \"method\": \"standard|express|pickup\",\n",
        "    \"preferred_by\": \"datetime\"\n",
        "  }\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXZL1_PrKpnQ"
      },
      "source": [
        "---\n",
        "\n",
        "# 1. Configuraci√≥n Inicial\n",
        "\n",
        "Esta secci√≥n prepara el entorno de ejecuci√≥n:\n",
        "- **Librer√≠as**: torch, transformers, PEFT para inferencia\n",
        "- **Dispositivo**: Detecci√≥n autom√°tica de CUDA/CPU\n",
        "- **Semillas**: Reproducibilidad de resultados\n",
        "- **Par√°metros**: Configuraci√≥n del modelo y generaci√≥n\n",
        "\n",
        "**Nota**: Este notebook requiere que `train.ipynb` haya sido ejecutado previamente para generar el modelo entrenado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5rpkyP4KpnR"
      },
      "source": [
        "## 1.1 Importar Librer√≠as\n",
        "\n",
        "**Librer√≠as principales:**\n",
        "- `torch`: Framework de deep learning\n",
        "- `transformers`: Modelos pre-entrenados (Qwen3-0.6B)\n",
        "- `peft`: Parameter-Efficient Fine-Tuning (LoRA)\n",
        "- `shared_functions`: Funciones personalizadas para construcci√≥n de prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bhKz_NgWKpnR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM\n",
        ")\n",
        "from peft import PeftModel, LoraConfig\n",
        "import shared_functions as custom_sharfun  # Funciones para construir prompts\n",
        "import clean_fields_json as custom_clean_fields\n",
        "\n",
        "# Importar BitsAndBytesConfig solo si hay CUDA disponible\n",
        "if torch.cuda.is_available():\n",
        "    from transformers import BitsAndBytesConfig\n",
        "\n",
        "# Configuraci√≥n de warnings\n",
        "from transformers import logging as hf_logging\n",
        "hf_logging.set_verbosity_warning()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VgsLaecKpnR"
      },
      "source": [
        "## 1.2 Configuraci√≥n del Dispositivo y Semillas\n",
        "\n",
        "**Configuraci√≥n de reproducibilidad:**\n",
        "- Semilla fija (42) para resultados consistentes\n",
        "- Detecci√≥n autom√°tica de GPU (CUDA) o CPU\n",
        "- Configuraci√≥n de semillas para torch, random y numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLPUAg-ZKpnS",
        "outputId": "b2cfd5f7-2a54-4eb1-9838-8e6206c543a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Configuraci√≥n del dispositivo\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "GLB_SEED = 42\n",
        "torch.manual_seed(GLB_SEED)\n",
        "random.seed(GLB_SEED)\n",
        "np.random.seed(GLB_SEED)\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(GLB_SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prN0DLaWKpnS"
      },
      "source": [
        "## 1.3 Par√°metros de Configuraci√≥n\n",
        "\n",
        "**Par√°metros del modelo:**\n",
        "- `MAX_LENGTH`: 1600 tokens (longitud m√°xima de entrada)\n",
        "- `GEN_MAX_NEW_TOKENS`: 400 tokens (tokens m√°ximos a generar)\n",
        "- `MODEL_DIR`: Directorio con el modelo entrenado\n",
        "\n",
        "**Batch size adaptativo:**\n",
        "- GPU: 14 ejemplos por batch\n",
        "- CPU: 8 ejemplos por batch (reduce memoria pero es m√°s lento)\n",
        "\n",
        "** Importante**: Verifica que el directorio del modelo exista antes de continuar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ1G2uEJKpnS",
        "outputId": "6f5669b6-3b31-4cec-c7de-9f623586c94c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Modelo a usar: output/results/v11/modfinal_full\n",
            " Verificando si existe...\n",
            " Modelo encontrado en: c:\\ProjGIT\\01_GIT\\UniAndesPro\\Labs_Univer\\14_PLN2\\Micro 4\\output\\results\\v11\\modfinal_full\n",
            " Directorio de salida: mod_open_eval\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANTE: Configuraci√≥n adaptada para modfinal_v04\n",
        "# ------------------------------------------------------------------\n",
        "# Data / tokenization\n",
        "MAX_LENGTH = 1600\n",
        "GEN_MAX_NEW_TOKENS = 400\n",
        "\n",
        "# MODELO: Usar modfinal_v04 (tu modelo entrenado)\n",
        "OUTPUT_DIR = \"mod_open_eval\" # Para guardar resultados de open evaluation\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "ADAPTER_DIR = \"output/results/v11/modfinal_full\"  # Usar la versi√≥n v11 con adaptadores entrenados\n",
        "\n",
        "print(f\" Modelo a usar: {ADAPTER_DIR}\")\n",
        "print(f\" Verificando si existe...\")\n",
        "if os.path.exists(ADAPTER_DIR):\n",
        "    print(f\" Modelo encontrado en: {os.path.abspath(ADAPTER_DIR)}\")\n",
        "else:\n",
        "    print(f\" ERROR: No se encuentra {ADAPTER_DIR}\")\n",
        "    print(f\"   Directorios disponibles:\")\n",
        "    for item in os.listdir(\".\"):\n",
        "        if os.path.isdir(item) and \"mod\" in item.lower():\n",
        "            print(f\"   - {item}\")\n",
        "\n",
        "# Crear directorio de salida si no existe\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\" Directorio de salida: {OUTPUT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtGrov5-KpnT"
      },
      "source": [
        "---\n",
        "\n",
        "# 2. Carga del Modelo Entrenado\n",
        "\n",
        "## 2.1 Proceso de Carga\n",
        "\n",
        "El modelo se carga en 3 pasos:\n",
        "1. **Tokenizer**: Configuraci√≥n con `padding_side=\"left\"` (importante para generaci√≥n)\n",
        "2. **Modelo base**: Qwen3-0.6B con o sin cuantizaci√≥n seg√∫n dispositivo\n",
        "3. **Adaptadores LoRA**: Pesos entrenados aplicados al modelo base\n",
        "\n",
        "**Configuraci√≥n seg√∫n dispositivo:**\n",
        "- **GPU (CUDA)**: Modelo cuantizado 4-bit ‚Üí Reduce memoria ~60% con m√≠nima p√©rdida de calidad\n",
        "- **CPU**: Modelo en float32 ‚Üí Sin cuantizaci√≥n (m√°s lento pero funcional)\n",
        "\n",
        "**Modo evaluaci√≥n**: `model.eval()` desactiva dropout y pone BatchNorm en modo inferencia.\n",
        "\n",
        "** Nota**: Respetar `tokenizer.padding_side = \"left\"` es cr√≠tico para que la generaci√≥n funcione correctamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Cargando tokenizer...\n",
            "üîÑ Cargando modelo base en 4 bits (GPU)...\n",
            "Modelo base: Qwen/Qwen3-0.6B-Base\n",
            "Cargando modelo LoRA...\n",
            "‚úÖ Modelo cargado exitosamente en modo evaluaci√≥n\n"
          ]
        }
      ],
      "source": [
        "pt_file = os.path.join(ADAPTER_DIR, \"weights.pt\")\n",
        "pt_loaded = torch.load(pt_file, map_location=DEVICE, weights_only=False)\n",
        "\n",
        "print(\"üîß Cargando tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(pt_loaded[\"tokenizer\"])\n",
        "tokenizer.padding_side = \"left\" # A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "if DEVICE == \"cuda\":\n",
        "    print(\"üîÑ Cargando modelo base en 4 bits (GPU)...\")\n",
        "    bnb_config = BitsAndBytesConfig(**pt_loaded[\"bnb_config\"])\n",
        "\n",
        "    print(\"Modelo base:\", pt_loaded[\"model_id\"])\n",
        "    model_base = AutoModelForCausalLM.from_pretrained(\n",
        "        pt_loaded[\"model_id\"],\n",
        "        quantization_config=bnb_config, \n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  CPU detectado - Cargando modelo SIN cuantizaci√≥n (puede ser lento)...\")\n",
        "    model_base = AutoModelForCausalLM.from_pretrained(\n",
        "        pt_loaded[\"model_id\"],\n",
        "        dtype=torch.float32,\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "    model_base = model_base.to(DEVICE)\n",
        "\n",
        "print(\"Cargando modelo LoRA...\")\n",
        "# Debes usar la misma configuraci√≥n LoRA que usaste al entrenar\n",
        "config_lora = LoraConfig(\n",
        "    r = pt_loaded[\"config\"][\"lora_r\"],\n",
        "    lora_alpha = pt_loaded[\"config\"][\"lora_alpha\"],\n",
        "    lora_dropout = pt_loaded[\"config\"][\"lora_drop\"],\n",
        "    target_modules = pt_loaded[\"config\"][\"lora_target_mods\"],\n",
        "    bias=pt_loaded[\"config\"][\"lora_bias\"],\n",
        "    task_type=pt_loaded[\"config\"][\"lora_task_type\"]\n",
        ")\n",
        "\n",
        "model = PeftModel(model_base, config_lora)\n",
        "model.load_state_dict(pt_loaded[\"peft\"], strict=False)\n",
        "\n",
        "model.eval()\n",
        "print(\"‚úÖ Modelo cargado exitosamente en modo evaluaci√≥n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOqQCJj5KpnT"
      },
      "source": [
        "---\n",
        "\n",
        "# 3. Open Evaluation - Consultas Abiertas\n",
        "\n",
        "## 3.1 ¬øQu√© es Open Evaluation?\n",
        "\n",
        "**Open Evaluation** permite probar el modelo con **textos completamente nuevos** que nunca vio durante el entrenamiento.\n",
        "\n",
        "**Diferencias con evaluaci√≥n est√°ndar:**\n",
        "- **Evaluaci√≥n est√°ndar**: Dataset fijo con respuestas conocidas ‚Üí Mide F1 score\n",
        "- **Open Evaluation**: Textos libres sin respuestas conocidas ‚Üí Verifica generalizaci√≥n\n",
        "\n",
        "**Casos de uso:**\n",
        "- ‚úÖ Probar pedidos reales de clientes\n",
        "- ‚úÖ Evaluar robustez ante textos mal escritos o incompletos\n",
        "- ‚úÖ Demostrar capacidades del modelo a stakeholders\n",
        "- ‚úÖ Detectar fallos en casos edge (datos faltantes, formatos raros)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH6aFRs4KpnU"
      },
      "source": [
        "## 3.2 Funci√≥n Principal: `generar_orden_compra()`\n",
        "\n",
        "Esta funci√≥n es el punto de entrada para consultas abiertas:\n",
        "\n",
        "**Entrada:**\n",
        "- `texto_natural` (str): Pedido en lenguaje natural (email, SMS, etc.)\n",
        "- `verbose` (bool): Si es True, muestra el JSON generado formateado\n",
        "\n",
        "**Proceso interno:**\n",
        "1. **Construcci√≥n del prompt**: Usa `custom_sharfun.build_prompt_text_to_json()`\n",
        "2. **Tokenizaci√≥n**: Convierte texto ‚Üí tokens (m√°x 1500)\n",
        "3. **Generaci√≥n**: Modelo produce tokens nuevos (m√°x 377)\n",
        "4. **Extracci√≥n**: Busca y parsea el JSON en la respuesta\n",
        "5. **Validaci√≥n**: Verifica que sea JSON v√°lido\n",
        "\n",
        "**Salida:**\n",
        "- `dict`: JSON parseado si exitoso\n",
        "- `None`: Si no se pudo generar/parsear JSON\n",
        "\n",
        "** Importante**: Esta funci√≥n NO calcula F1 score porque no tiene \"ground truth\" para comparar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jP5o1VSSKpnU"
      },
      "outputs": [],
      "source": [
        "def generar_orden_compra(texto_natural: str, verbose: bool = True) -> dict:\n",
        "    \"\"\"\n",
        "    Genera una orden de compra en formato JSON a partir de texto en lenguaje natural.\n",
        "\n",
        "    Args:\n",
        "        texto_natural: Texto en lenguaje natural describiendo la orden de compra\n",
        "        verbose: Si True, imprime informaci√≥n adicional del proceso\n",
        "\n",
        "    Returns:\n",
        "        dict: JSON con la orden de compra estructurada\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" TEXTO DE ENTRADA:\")\n",
        "        print(\"=\"*80)\n",
        "        print(texto_natural)\n",
        "        print(\"\\n Generando JSON...\\n\")\n",
        "\n",
        "    # Generar el JSON usando el modelo personalizado\n",
        "    raw_output = custom_sharfun.generate_json_raw(\n",
        "        text=texto_natural,\n",
        "        max_new_tokens=GEN_MAX_NEW_TOKENS,\n",
        "        max_length=MAX_LENGTH,\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "        device=DEVICE\n",
        "    )\n",
        "\n",
        "    # Extraer JSON del texto generado\n",
        "    json_obj = custom_sharfun.extract_json_from_text(raw_output)\n",
        "    json_obj = custom_clean_fields.normalize_example_json_pred(json_obj)\n",
        "\n",
        "    if json_obj is None:\n",
        "        if verbose:\n",
        "            print(\" No se pudo extraer un JSON v√°lido\")\n",
        "            print(\"\\n Salida cruda del modelo:\")\n",
        "            print(raw_output)\n",
        "        return {}\n",
        "\n",
        "    if verbose:\n",
        "        print(\"=\"*80)\n",
        "        print(\" JSON GENERADO:\")\n",
        "        print(\"=\"*80)\n",
        "        print(json.dumps(json_obj, indent=2, ensure_ascii=False))\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    return json_obj\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMFdA6F9KpnU"
      },
      "source": [
        "## 3.4 Ejemplos de Uso\n",
        "\n",
        "### Ejemplo 1: Pedido Completo y Bien Estructurado\n",
        "\n",
        "**Caso:** Cliente proporciona toda la informaci√≥n necesaria de forma clara.\n",
        "\n",
        "**Expectativa:** El modelo debe extraer correctamente:\n",
        "- Nombre y contacto del comprador\n",
        "- Lista de productos con cantidades\n",
        "- Direcci√≥n de env√≠o completa\n",
        "- M√©todo de env√≠o y fecha preferida\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZl7LPHIKpnU",
        "outputId": "4580380b-4b67-4f2f-b5e2-33fab5979c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            " TEXTO DE ENTRADA:\n",
            "================================================================================\n",
            "\n",
            "Hola, necesito hacer un pedido urgente.\n",
            "\n",
            "Mi nombre es Mar√≠a Garc√≠a y mi correo es maria.garcia@example.com\n",
            "Mi tel√©fono es +34-555-123-456\n",
            "\n",
            "Quiero comprar:\n",
            "- 2 teclados inal√°mbricos\n",
            "- 1 mouse ergon√≥mico\n",
            "\n",
            "Mi direcci√≥n es: Calle Mayor 123, Madrid, Espa√±a, c√≥digo postal 28013\n",
            "\n",
            "Necesito env√≠o express para antes del 15 de diciembre de 2025.\n",
            "\n",
            "Gracias!\n",
            "\n",
            "\n",
            " Generando JSON...\n",
            "\n",
            "================================================================================\n",
            " JSON GENERADO:\n",
            "================================================================================\n",
            "{\n",
            "  \"buyer\": {\n",
            "    \"name\": null,\n",
            "    \"email\": \"maria.garcia@example.com\",\n",
            "    \"contact\": {\n",
            "      \"phone\": \"+34-555-123-456\",\n",
            "      \"alt_email\": null,\n",
            "      \"preferred_contact\": null\n",
            "    },\n",
            "    \"addresses\": [\n",
            "      {\n",
            "        \"street\": \"Calle Mayor 123\",\n",
            "        \"city\": \"Madrid\",\n",
            "        \"state\": null,\n",
            "        \"postal_code\": \"28013\",\n",
            "        \"country\": \"ES\"\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  \"purchases\": [\n",
            "    {\n",
            "      \"product_name\": \"Tecladito inal√°mbrico \",\n",
            "      \"quantity\": 2,\n",
            "      \"currency\": null,\n",
            "      \"discount_code\": null\n",
            "    },\n",
            "    {\n",
            "      \"product_name\": \"Mouse ergon√≥mico \",\n",
            "      \"quantity\": 1,\n",
            "      \"currency\": null,\n",
            "      \"discount_code\": null\n",
            "    }\n",
            "  ],\n",
            "  \"shipping\": {\n",
            "    \"method\": \"express\",\n",
            "    \"preferred_by\": \"2025-12-15T00:00:00\"\n",
            "  }\n",
            "}\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "texto_ejemplo_1 = \"\"\"\n",
        "Hola, necesito hacer un pedido urgente.\n",
        "\n",
        "Mi nombre es Mar√≠a Garc√≠a y mi correo es maria.garcia@example.com\n",
        "Mi tel√©fono es +34-555-123-456\n",
        "\n",
        "Quiero comprar:\n",
        "- 2 teclados inal√°mbricos\n",
        "- 1 mouse ergon√≥mico\n",
        "\n",
        "Mi direcci√≥n es: Calle Mayor 123, Madrid, Espa√±a, c√≥digo postal 28013\n",
        "\n",
        "Necesito env√≠o express para antes del 15 de diciembre de 2025.\n",
        "\n",
        "Gracias!\n",
        "\"\"\"\n",
        "\n",
        "resultado_1 = generar_orden_compra(texto_ejemplo_1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wezgg4bKpnU"
      },
      "source": [
        "### Ejemplo 2: Pedido con Informaci√≥n Incompleta\n",
        "\n",
        "**Caso:** Cliente omite algunos detalles (sin email, sin tel√©fono, sin fecha).\n",
        "\n",
        "**Expectativa:** El modelo debe:\n",
        "- Extraer lo que est√° disponible\n",
        "- Usar valores por defecto razonables para lo faltante\n",
        "- Mantener la estructura JSON v√°lida\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HuE5u9PKpnV",
        "outputId": "0f00d758-eb34-4287-c443-a7c69457c79e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            " TEXTO DE ENTRADA:\n",
            "================================================================================\n",
            "\n",
            "Hola, quiero ordenar 5 laptops para mi oficina.\n",
            "Puedes contactarme al correo: juan.perez@company.com\n",
            "Prefiero recogerlo yo mismo en la tienda.\n",
            "\n",
            "\n",
            " Generando JSON...\n",
            "\n",
            "================================================================================\n",
            " JSON GENERADO:\n",
            "================================================================================\n",
            "{\n",
            "  \"buyer\": {\n",
            "    \"name\": null,\n",
            "    \"email\": \"juan.perez@company.com\",\n",
            "    \"contact\": {\n",
            "      \"phone\": null,\n",
            "      \"alt_email\": null,\n",
            "      \"preferred_contact\": \"email\"\n",
            "    },\n",
            "    \"addresses\": null\n",
            "  },\n",
            "  \"purchases\": [\n",
            "    {\n",
            "      \"product_name\": \"Laptop\",\n",
            "      \"quantity\": 5,\n",
            "      \"currency\": null,\n",
            "      \"discount_code\": null\n",
            "    }\n",
            "  ],\n",
            "  \"shipping\": {\n",
            "    \"method\": \"pickup\",\n",
            "    \"preferred_by\": null\n",
            "  }\n",
            "}\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "texto_ejemplo_2 = \"\"\"\n",
        "Hola, quiero ordenar 5 laptops para mi oficina.\n",
        "Puedes contactarme al correo: juan.perez@company.com\n",
        "Prefiero recogerlo yo mismo en la tienda.\n",
        "\"\"\"\n",
        "\n",
        "resultado_2 = generar_orden_compra(texto_ejemplo_2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK_D3nXzKpnV"
      },
      "source": [
        "### Ejemplo 3: Tu Propia Consulta\n",
        "\n",
        "**Instrucciones:**\n",
        "1. Modifica el texto en la celda siguiente\n",
        "2. Ejecuta la celda para ver el JSON generado\n",
        "3. Observa c√≥mo el modelo interpreta tu texto\n",
        "\n",
        "** Tips para mejores resultados:**\n",
        "- Incluye nombre del comprador\n",
        "- Especifica productos y cantidades claramente\n",
        "- Menciona direcci√≥n de env√≠o\n",
        "- Indica m√©todo de env√≠o si es importante\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTHXqhSRKpnV",
        "outputId": "1f80e0c5-1ec7-4d53-e089-7e14e7b3c7c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            " TEXTO DE ENTRADA:\n",
            "================================================================================\n",
            "\n",
            "Escribe aqu√≠ tu orden de compra en lenguaje natural...\n",
            "\n",
            "\n",
            " Generando JSON...\n",
            "\n",
            "================================================================================\n",
            " JSON GENERADO:\n",
            "================================================================================\n",
            "{\n",
            "  \"buyer\": {\n",
            "    \"name\": null,\n",
            "    \"email\": null,\n",
            "    \"contact\": null,\n",
            "    \"addresses\": null\n",
            "  },\n",
            "  \"purchases\": [\n",
            "    {\n",
            "      \"product_name\": \"Pasta Spaghetti Doria \",\n",
            "      \"quantity\": 1,\n",
            "      \"currency\": null,\n",
            "      \"discount_code\": null\n",
            "    }\n",
            "  ],\n",
            "  \"shipping\": null\n",
            "}\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "#  Modifica el texto a continuaci√≥n con tu propia orden de compra\n",
        "mi_texto = \"\"\"\n",
        "Escribe aqu√≠ tu orden de compra en lenguaje natural...\n",
        "\"\"\"\n",
        "\n",
        "mi_resultado = generar_orden_compra(mi_texto)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yurur0_MKpnV"
      },
      "source": [
        "---\n",
        "\n",
        "## 3.4 Consulta Interactiva\n",
        "\n",
        "**Modo interactivo**: Ejecuta esta celda para ingresar tu propia consulta desde el teclado.\n",
        "\n",
        "**Casos de prueba sugeridos:**\n",
        "- Pedido en lenguaje informal (como SMS)\n",
        "- Pedido con errores de ortograf√≠a\n",
        "- Pedido con m√∫ltiples productos\n",
        "- Pedido internacional (diferentes pa√≠ses)\n",
        "- Pedido urgente vs. env√≠o est√°ndar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUcybpQWKpnV",
        "outputId": "7355970f-f163-4bb1-9608-872433d6100b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            " TEXTO DE ENTRADA:\n",
            "================================================================================\n",
            "quiero 5 almendras y 8 botellas de vino para dormir\n",
            "\n",
            " Generando JSON...\n",
            "\n",
            "================================================================================\n",
            " JSON GENERADO:\n",
            "================================================================================\n",
            "{\n",
            "  \"buyer\": {\n",
            "    \"name\": null,\n",
            "    \"email\": null,\n",
            "    \"contact\": null,\n",
            "    \"addresses\": null\n",
            "  },\n",
            "  \"purchases\": [\n",
            "    {\n",
            "      \"product_name\": \"Almendra \",\n",
            "      \"quantity\": 5,\n",
            "      \"currency\": null,\n",
            "      \"discount_code\": null\n",
            "    },\n",
            "    {\n",
            "      \"product_name\": \"Vino para dormir \",\n",
            "      \"quantity\": 8,\n",
            "      \"currency\": null,\n",
            "      \"discount_code\": null\n",
            "    }\n",
            "  ],\n",
            "  \"shipping\": null\n",
            "}\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Escribe tu consulta aqu√≠ y ejecuta la celda\n",
        "consulta = input(\"Escribe tu orden de compra en lenguaje natural:\\n\")\n",
        "resultado = generar_orden_compra(consulta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieXONTPoKpnV"
      },
      "source": [
        "---\n",
        "\n",
        "## 3.5 Guardar Resultados\n",
        "\n",
        "**Exportaci√≥n de resultados**: Si necesitas guardar el JSON generado para su posterior uso.\n",
        "\n",
        "**Formato de salida:**\n",
        "- Archivo `.json` con indentaci√≥n legible\n",
        "- Codificaci√≥n UTF-8 (soporta caracteres especiales)\n",
        "- Nombre personalizable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_h8VjjTKpnV",
        "outputId": "942df829-e6ed-4098-f547-67c8a8f76257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resultado guardado en: mod_open_eval\\mi_orden_compra.json\n"
          ]
        }
      ],
      "source": [
        "# Guardar el √∫ltimo resultado generado\n",
        "\n",
        "output_file = os.path.join(OUTPUT_DIR, \"mi_orden_compra.json\")\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(resultado, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Resultado guardado en: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG1Y0B50KpnV"
      },
      "source": [
        "---\n",
        "\n",
        "##  Troubleshooting y Tips\n",
        "\n",
        "### Problemas Comunes\n",
        "\n",
        "**1. El modelo genera texto pero no JSON v√°lido**\n",
        "- **Causa**: Prompt poco claro o texto de entrada muy ambiguo\n",
        "- **Soluci√≥n**: Agrega m√°s detalles (nombre, productos, direcci√≥n)\n",
        "\n",
        "**2. JSON incompleto o con campos vac√≠os**\n",
        "- **Causa**: Informaci√≥n faltante en el texto de entrada\n",
        "- **Soluci√≥n**: El modelo usa valores por defecto; verifica si son aceptables\n",
        "\n",
        "**3. Generaci√≥n muy lenta (CPU)**\n",
        "- **Causa**: CPU es ~10x m√°s lento que GPU\n",
        "- **Soluci√≥n**: Normal; considera usar GPU o reducir `MAX_LENGTH`\n",
        "\n",
        "**4. Error \"CUDA out of memory\"**\n",
        "- **Causa**: GPU sin memoria suficiente\n",
        "- **Soluci√≥n**: Reinicia el kernel o reduce `BATCH_SIZE`\n",
        "\n",
        "### Tips de Mejores Pr√°cticas\n",
        "\n",
        " **Para entregas del proyecto:**\n",
        "- Incluye al menos 4 ejemplos variados\n",
        "- Muestra casos exitosos Y casos con informaci√≥n incompleta\n",
        "- Comenta los resultados obtenidos\n",
        "\n",
        " **Para uso real:**\n",
        "- Valida el JSON generado antes de usarlo\n",
        "- Ten un fallback para casos donde el modelo falla\n",
        "- Loguea consultas que fallan para mejorar el modelo\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYhaJsXkKpnV"
      },
      "source": [
        "---\n",
        "\n",
        "## üìö Resumen del Notebook\n",
        "\n",
        "###  Lo que completamos\n",
        "\n",
        "1. **Configuraci√≥n**: Cargamos el modelo entrenado con LoRA\n",
        "2. **Funci√≥n principal**: `generar_orden_compra()` lista para usar\n",
        "3. **Ejemplos**: Probamos 3+ casos de uso diferentes\n",
        "4. **Interactividad**: Modo para probar tus propias consultas\n",
        "5. **Exportaci√≥n**: Guardar resultados en JSON\n",
        "\n",
        "###  Pr√≥ximos Pasos\n",
        "\n",
        "Para completar el proyecto, aseg√∫rate de tener:\n",
        "- [] `train.ipynb` - Entrenamiento completado\n",
        "- [] `weights.pt` - Modelo guardado\n",
        "- [] `evaluation.ipynb` - Evaluaci√≥n en test set con F1 score\n",
        "- [] **`open-evaluation.ipynb`** - Este notebook completado\n",
        "\n",
        "###  Referencias\n",
        "\n",
        "**Documentos del proyecto:**\n",
        "- `Enunciado_competencia_MANLP_2025_02.pdf`\n",
        "- `evaluation_metric.py`\n",
        "- `shared_functions.py`\n",
        "- `clean_fields_json.py`\n",
        "\n",
        "**Modelos y datos:**\n",
        "- Modelo base: `Qwen/Qwen3-0.6B-Base`\n",
        "- Pesos del modelo y otras configuraciones: `output/results/v11/modfinal_full`\n",
        "- Dataset: `data/train/` y `data/eval/`\n",
        "\n",
        "---\n",
        "\n",
        "**¬°Notebook completado exitosamente!**\n",
        "\n",
        "*Este notebook demuestra la capacidad del modelo para generalizar a textos no vistos durante el entrenamiento.*\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python_312_CUDA",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
