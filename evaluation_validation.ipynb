{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación Completa - Validación y Consultas Abiertas\n",
    "\n",
    "Este notebook combina:\n",
    "1. Validación de datos con el 10% de los datos generados por train.ipynb\n",
    "2. Consultas abiertas en lenguaje natural\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Validación de Datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import List\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shared_functions as custom_sharfun  #el archivo .py con funciones compartidas\n",
    "import evaluation_metric as custom_metrics # es la ultima version del archivo de metricas\n",
    "\n",
    "from peft import PeftModel\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_warning()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del dispositivo y semillas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuración del dispositivo\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "GLB_SEED = 42\n",
    "torch.manual_seed(GLB_SEED)\n",
    "random.seed(GLB_SEED)\n",
    "np.random.seed(GLB_SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(GLB_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros de configuración\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: dejar las mismas configuraciones que train.ipynb\n",
    "# ------------------------------------------------------------------\n",
    "# Evaluación\n",
    "BATCH_SIZE_EVAL = 14 ## ajuste aqui segun tamaño GPU. puede ser que le toque bajar a 12 o 8\n",
    "\n",
    "# Data / tokenization\n",
    "MAX_LENGTH = 1500 # Obtenido al percentil 99% de training data\n",
    "GEN_MAX_NEW_TOKENS = 377 # Ajustado a partir de analisis de percentiles\n",
    "\n",
    "# Directorios y modelos\n",
    "OUTPUT_DIR = \"output/results/v09\"\n",
    "ADAPTER_DIR = os.path.join(OUTPUT_DIR, \"modfinal\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando el modelo entrenado\n",
    "\n",
    "**Nota importante:** Respetar `tokenizer.padding_side = \"left\"` y `model.eval()` para que entre en modo evaluación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando tokenizer...\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'output/results/v09/modfinal'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/new_entorno_2/lib/python3.11/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     hf_hub_download(\n\u001b[32m    480\u001b[39m         path_or_repo_id,\n\u001b[32m    481\u001b[39m         filenames[\u001b[32m0\u001b[39m],\n\u001b[32m    482\u001b[39m         subfolder=\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[32m    483\u001b[39m         repo_type=repo_type,\n\u001b[32m    484\u001b[39m         revision=revision,\n\u001b[32m    485\u001b[39m         cache_dir=cache_dir,\n\u001b[32m    486\u001b[39m         user_agent=user_agent,\n\u001b[32m    487\u001b[39m         force_download=force_download,\n\u001b[32m    488\u001b[39m         proxies=proxies,\n\u001b[32m    489\u001b[39m         resume_download=resume_download,\n\u001b[32m    490\u001b[39m         token=token,\n\u001b[32m    491\u001b[39m         local_files_only=local_files_only,\n\u001b[32m    492\u001b[39m     )\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/new_entorno_2/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     validate_repo_id(arg_value)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/new_entorno_2/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:154\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m repo_id.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'output/results/v09/modfinal'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCargando tokenizer...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR)\n\u001b[32m      3\u001b[39m tokenizer.padding_side = \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/new_entorno_2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:1073\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1070\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[32m   1072\u001b[39m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[32m   1075\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = tokenizer_config[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/new_entorno_2/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:905\u001b[39m, in \u001b[36mget_tokenizer_config\u001b[39m\u001b[34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[39m\n\u001b[32m    902\u001b[39m     token = use_auth_token\n\u001b[32m    904\u001b[39m commit_hash = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m resolved_config_file = cached_file(\n\u001b[32m    906\u001b[39m     pretrained_model_name_or_path,\n\u001b[32m    907\u001b[39m     TOKENIZER_CONFIG_FILE,\n\u001b[32m    908\u001b[39m     cache_dir=cache_dir,\n\u001b[32m    909\u001b[39m     force_download=force_download,\n\u001b[32m    910\u001b[39m     resume_download=resume_download,\n\u001b[32m    911\u001b[39m     proxies=proxies,\n\u001b[32m    912\u001b[39m     token=token,\n\u001b[32m    913\u001b[39m     revision=revision,\n\u001b[32m    914\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    915\u001b[39m     subfolder=subfolder,\n\u001b[32m    916\u001b[39m     _raise_exceptions_for_gated_repo=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    917\u001b[39m     _raise_exceptions_for_missing_entries=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    918\u001b[39m     _raise_exceptions_for_connection_errors=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    919\u001b[39m     _commit_hash=commit_hash,\n\u001b[32m    920\u001b[39m )\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    922\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/new_entorno_2/lib/python3.11/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/new_entorno_2/lib/python3.11/site-packages/transformers/utils/hub.py:531\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    525\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m resolved_files = [\n\u001b[32m    532\u001b[39m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)\n\u001b[32m    533\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[32m    534\u001b[39m ]\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/new_entorno_2/lib/python3.11/site-packages/transformers/utils/hub.py:532\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    525\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPermissionError at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m when downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    526\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck cache directory permissions. Common causes: 1) another user is downloading the same model (please wait); \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m2) a previous download was canceled and the lock file needs manual removal.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[32m    531\u001b[39m resolved_files = [\n\u001b[32m--> \u001b[39m\u001b[32m532\u001b[39m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)\n\u001b[32m    533\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[32m    534\u001b[39m ]\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/new_entorno_2/lib/python3.11/site-packages/transformers/utils/hub.py:143\u001b[39m, in \u001b[36m_get_cache_file_to_return\u001b[39m\u001b[34m(path_or_repo_id, full_filename, cache_dir, revision, repo_type)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cache_file_to_return\u001b[39m(\n\u001b[32m    136\u001b[39m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    137\u001b[39m     full_filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    141\u001b[39m ):\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     resolved_file = try_to_load_from_cache(\n\u001b[32m    144\u001b[39m         path_or_repo_id, full_filename, cache_dir=cache_dir, revision=revision, repo_type=repo_type\n\u001b[32m    145\u001b[39m     )\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resolved_file != _CACHED_NO_EXIST:\n\u001b[32m    147\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/new_entorno_2/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mzip\u001b[39m(signature.parameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[32m    103\u001b[39m     kwargs.items(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[32m    104\u001b[39m ):\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         validate_repo_id(arg_value)\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    109\u001b[39m         has_token = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/new_entorno_2/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:154\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m repo_id.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m--\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m\u001b[33m are\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m forbidden, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m cannot start or end the name, max length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': 'output/results/v09/modfinal'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "print(\"Cargando tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR)\n",
    "tokenizer.padding_side = \"left\" # A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Cargando modelo base en 4 bits...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",      # NF4 = mejor precisión\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B-Base\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Cargando modelo...\")\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar datos de validación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory output/results/v09/datavalidation not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m OUTPUT_DIR_VALDATA = os.path.join(OUTPUT_DIR, \u001b[33m\"\u001b[39m\u001b[33mdatavalidation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m val_list = load_from_disk(OUTPUT_DIR_VALDATA)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/new_entorno_2/lib/python3.11/site-packages/datasets/load.py:1471\u001b[39m, in \u001b[36mload_from_disk\u001b[39m\u001b[34m(dataset_path, keep_in_memory, storage_options)\u001b[39m\n\u001b[32m   1469\u001b[39m fs, *_ = url_to_fs(dataset_path, **(storage_options \u001b[38;5;129;01mor\u001b[39;00m {}))\n\u001b[32m   1470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs.exists(dataset_path):\n\u001b[32m-> \u001b[39m\u001b[32m1471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fs.isfile(posixpath.join(dataset_path, config.DATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs.isfile(\n\u001b[32m   1473\u001b[39m     posixpath.join(dataset_path, config.DATASET_STATE_JSON_FILENAME)\n\u001b[32m   1474\u001b[39m ):\n\u001b[32m   1475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset.load_from_disk(dataset_path, keep_in_memory=keep_in_memory, storage_options=storage_options)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Directory output/results/v09/datavalidation not found"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR_VALDATA = os.path.join(OUTPUT_DIR, \"datavalidation\")\n",
    "val_list = load_from_disk(OUTPUT_DIR_VALDATA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones adaptadas para ejecutar en lotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_raw_batch( texts: List[str], tokenizer, model, device, max_new_tokens: int, max_length: int, batch_size: int = 8):\n",
    "    outputs = []\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    eos_brace_id = tokenizer.encode(\"}\", add_special_tokens=False)[0]\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating\", total=math.ceil(len(texts)/batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        prompts = [custom_sharfun.build_prompt(t) for t in batch]\n",
    "\n",
    "        enc = tokenizer( prompts, return_tensors='pt', truncation=True, padding=\"longest\", max_length=max_length).to(device)\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        attention_mask = enc[\"attention_mask\"]\n",
    "        \n",
    "\n",
    "        with torch.inference_mode():\n",
    "            model.eval()\n",
    "            out = model.generate( input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, do_sample=False, pad_token_id=pad_id, eos_token_id=eos_brace_id, use_cache=True )\n",
    "\n",
    "        decoded = tokenizer.batch_decode(out, skip_special_tokens=True) # Decodificar outputs en lote\n",
    "\n",
    "        # Recorte final\n",
    "        cleaned = []\n",
    "        for d in decoded:\n",
    "            if \"{\" in d and \"}\" in d:\n",
    "                first = d.find(\"{\")\n",
    "                last = d.rfind(\"}\")\n",
    "                d = d[first:last+1]\n",
    "            cleaned.append(d)\n",
    "\n",
    "        outputs.extend(cleaned)\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar texto bruto desde el modelo - EN MODO EVALUACIÓN\n",
    "def generate_json_raw(text: str, max_new_tokens: int, max_length: int, tokenizer, model, device):\n",
    "    prompt = custom_sharfun.build_prompt(text)\n",
    "    enc = tokenizer( prompt, return_tensors='pt', truncation=True, padding=\"longest\", max_length=max_length).to(device)\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    attention_mask = enc[\"attention_mask\"]\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id    \n",
    "    eos_brace_id = tokenizer.encode(\"}\", add_special_tokens=False)[0] # Forzar fin al cerrar JSON\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model.generate( input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, do_sample=False, pad_token_id=pad_id, eos_token_id=eos_brace_id, use_cache=True\n",
    "                             , temperature=0.0 )\n",
    "\n",
    "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    # EXTRA: recortar SOLO el JSON final.\n",
    "    if \"{\" in decoded and \"}\" in decoded:\n",
    "        first = decoded.find(\"{\")\n",
    "        last = decoded.rfind(\"}\")\n",
    "        decoded = decoded[first : last + 1]\n",
    "\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_text(text: str):\n",
    "    \"\"\"\n",
    "    Extrae el PRIMER JSON válido encontrado DESPUÉS del marcador '\\nJSON:\\n'.\n",
    "    Escanea llaves para encontrar un bloque JSON bien balanceado.\n",
    "    \"\"\"\n",
    "    marker = '{\"buyer\":'\n",
    "    pos = text.find(marker)\n",
    "    if pos != -1:\n",
    "        start = text.find(marker)\n",
    "    else: # pos == -1\n",
    "        marker = \"\\nJSON:\\n\"\n",
    "        pos = text.find(marker)\n",
    "        if pos == -1:\n",
    "            start = text.find(\"{\")\n",
    "        else:\n",
    "            start = text.find(\"{\", pos + len(marker)) # Buscar la primera llave '{' después del marcador\n",
    "            if start == -1:\n",
    "                start = text.find(\"{\")\n",
    "\n",
    "    if start == -1:\n",
    "        return None\n",
    "\n",
    "    brace_count = 0\n",
    "    in_json = False\n",
    "\n",
    "    for i in range(start, len(text)):\n",
    "        if text[i] == \"{\":\n",
    "            brace_count += 1\n",
    "            in_json = True\n",
    "        elif text[i] == \"}\":\n",
    "            brace_count -= 1\n",
    "\n",
    "            # Si brace_count llega a 0 => JSON completo\n",
    "            if in_json and brace_count == 0:\n",
    "                candidate = text[start:i+1]\n",
    "\n",
    "                # intentar parsear\n",
    "                try:\n",
    "                    return json.loads(candidate)\n",
    "                except json.JSONDecodeError:\n",
    "                    # intento reemplazando comillas simples\n",
    "                    try:\n",
    "                        return json.loads(candidate.replace(\"'\", '\"'))\n",
    "                    except Exception:\n",
    "                        return None\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar validación en lotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m start_time = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m texts = [ex[\u001b[33m\"\u001b[39m\u001b[33mnatural_language\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m val_list]\n\u001b[32m      4\u001b[39m true_jsons = [ex[\u001b[33m\"\u001b[39m\u001b[33mjson_data\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m val_list]\n\u001b[32m      5\u001b[39m nat_langs = texts\n",
      "\u001b[31mNameError\u001b[39m: name 'val_list' is not defined"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "texts = [ex[\"natural_language\"] for ex in val_list]\n",
    "true_jsons = [ex[\"json_data\"] for ex in val_list]\n",
    "nat_langs = texts\n",
    "\n",
    "pred_raw_list = generate_json_raw_batch( texts=texts, tokenizer=tokenizer, model=model, device=DEVICE, max_new_tokens=GEN_MAX_NEW_TOKENS,  max_length=MAX_LENGTH, batch_size=BATCH_SIZE_EVAL )\n",
    "results = []\n",
    "for nat_langs_save, raw, true_json in zip(nat_langs, pred_raw_list, true_jsons):\n",
    "    pred_obj = extract_json_from_text(raw)\n",
    "    if pred_obj is None:\n",
    "        pred_obj = {}\n",
    "    \n",
    "    f1 = 0.0\n",
    "    if pred_obj is None:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        try:\n",
    "            f1 = custom_metrics.evaluate_json(true_json, json.dumps(pred_obj, ensure_ascii=False))\n",
    "        except Exception:\n",
    "            f1 = float(1.0 if pred_obj == true_json else 0.0)\n",
    "\n",
    "    results.append({\n",
    "        \"nat_language\": nat_langs_save,\n",
    "        \"raw_prediction\": raw,\n",
    "        \"prediction\": pred_obj,\n",
    "        \"true_json\": true_json,\n",
    "        \"f1_score\": f1\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "print( custom_sharfun.print_time_execution(\"Etapa validación de datos\", start_time, end_time) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m f1_scores = [r[\u001b[33m'\u001b[39m\u001b[33mf1_score\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[32m      3\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m4\u001b[39m))\n\u001b[32m      4\u001b[39m plt.plot(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(f1_scores) + \u001b[32m1\u001b[39m), f1_scores, marker=\u001b[33m'\u001b[39m\u001b[33mo\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    " # Resumen de resultados en test: ver histrograma de F1 scores\n",
    "f1_scores = [r['f1_score'] for r in results]\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(f1_scores) + 1), f1_scores, marker='o')\n",
    "plt.title(\"F1 Scores por Ejemplo de Validación\")\n",
    "plt.xlabel(\"Ejemplo de Validación\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m     writer = csv.DictWriter(f, fieldnames=[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mpred\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m'\u001b[39m], delimiter=\u001b[33m'\u001b[39m\u001b[33m|\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m     writer.writeheader()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m      9\u001b[39m         writer.writerow({\n\u001b[32m     10\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m: r[\u001b[33m'\u001b[39m\u001b[33mnat_language\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     11\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m'\u001b[39m: r[\u001b[33m'\u001b[39m\u001b[33mf1_score\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m'\u001b[39m: json.dumps(r[\u001b[33m'\u001b[39m\u001b[33mtrue_json\u001b[39m\u001b[33m'\u001b[39m], ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     15\u001b[39m         })\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mCSV guardado en\u001b[39m\u001b[33m'\u001b[39m, csv_path)\n",
      "\u001b[31mNameError\u001b[39m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Guardar CSV - resultados de validación\n",
    "OUTPUT_DIR_VAL = os.path.join(OUTPUT_DIR, \"result_validation\")\n",
    "os.makedirs(OUTPUT_DIR_VAL, exist_ok=True)\n",
    "csv_path = os.path.join(OUTPUT_DIR_VAL, 'validation_results.csv')\n",
    "with open(csv_path, 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['text','f1','raw','pred','true'], delimiter='|')\n",
    "    writer.writeheader()\n",
    "    for r in results:\n",
    "        writer.writerow({\n",
    "            'text': r['nat_language'],\n",
    "            'f1': r['f1_score'],\n",
    "            'raw': r['raw_prediction'],\n",
    "            'pred': json.dumps(r['prediction'], ensure_ascii=False),\n",
    "            'true': json.dumps(r['true_json'], ensure_ascii=False)\n",
    "        })\n",
    "print('CSV guardado en', csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Histograma F1\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m f1_scores = [r[\u001b[33m'\u001b[39m\u001b[33mf1_score\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[32m      3\u001b[39m plt.figure()\n\u001b[32m      4\u001b[39m plt.hist(f1_scores, bins=\u001b[32m10\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Histograma F1\n",
    "f1_scores = [r['f1_score'] for r in results]\n",
    "plt.figure()\n",
    "plt.hist(f1_scores, bins=10)\n",
    "plt.title('Distribución de F1')\n",
    "plt.xlabel('F1')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'f1_distribution.png'))\n",
    "plt.show()\n",
    "plt.close()\n",
    "print('Histograma guardado en', os.path.join(OUTPUT_DIR, 'f1_distribution.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Parte 2: Open Evaluation - Consulta Abierta en Lenguaje Natural\n",
    "\n",
    "Esta sección permite realizar consultas abiertas en lenguaje natural y obtener una respuesta JSON generada por el modelo entrenado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Consulta Abierta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_orden_compra(texto_natural: str, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Genera una orden de compra en formato JSON a partir de texto en lenguaje natural.\n",
    "    \n",
    "    Args:\n",
    "        texto_natural: Texto en lenguaje natural describiendo la orden de compra\n",
    "        verbose: Si True, imprime información adicional del proceso\n",
    "    \n",
    "    Returns:\n",
    "        dict: JSON con la orden de compra estructurada\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" TEXTO DE ENTRADA:\")\n",
    "        print(\"=\"*80)\n",
    "        print(texto_natural)\n",
    "        print(\"\\n Generando JSON...\\n\")\n",
    "    \n",
    "    # Generar el JSON usando el modelo\n",
    "    raw_output = generate_json_raw(\n",
    "        text=texto_natural,\n",
    "        max_new_tokens=GEN_MAX_NEW_TOKENS,\n",
    "        max_length=MAX_LENGTH,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # Extraer JSON del texto generado\n",
    "    json_obj = extract_json_from_text(raw_output)\n",
    "    \n",
    "    if json_obj is None:\n",
    "        if verbose:\n",
    "            print(\" No se pudo extraer un JSON válido\")\n",
    "            print(\"\\n Salida cruda del modelo:\")\n",
    "            print(raw_output)\n",
    "        return {}\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*80)\n",
    "        print(\" JSON GENERADO:\")\n",
    "        print(\"=\"*80)\n",
    "        print(json.dumps(json_obj, indent=2, ensure_ascii=False))\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    return json_obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos de Uso\n",
    "\n",
    "#### Ejemplo 1: Pedido Simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " TEXTO DE ENTRADA:\n",
      "================================================================================\n",
      "\n",
      "Hola, necesito hacer un pedido urgente.\n",
      "\n",
      "Mi nombre es María García y mi correo es maria.garcia@example.com\n",
      "Mi teléfono es +34-555-123-456\n",
      "\n",
      "Quiero comprar:\n",
      "- 2 teclados inalámbricos\n",
      "- 1 mouse ergonómico\n",
      "\n",
      "Mi dirección es: Calle Mayor 123, Madrid, España, código postal 28013\n",
      "\n",
      "Necesito envío express para antes del 15 de diciembre de 2025.\n",
      "\n",
      "Gracias!\n",
      "\n",
      "\n",
      " Generando JSON...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      1\u001b[39m texto_ejemplo_1 = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33mHola, necesito hacer un pedido urgente.\u001b[39m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m \u001b[33mGracias!\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m resultado_1 = generar_orden_compra(texto_ejemplo_1)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mgenerar_orden_compra\u001b[39m\u001b[34m(texto_natural, verbose)\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Generando JSON...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Generar el JSON usando el modelo\u001b[39;00m\n\u001b[32m     20\u001b[39m raw_output = generate_json_raw(\n\u001b[32m     21\u001b[39m     text=texto_natural,\n\u001b[32m     22\u001b[39m     max_new_tokens=GEN_MAX_NEW_TOKENS,\n\u001b[32m     23\u001b[39m     max_length=MAX_LENGTH,\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     tokenizer=tokenizer,\n\u001b[32m     25\u001b[39m     model=model,\n\u001b[32m     26\u001b[39m     device=DEVICE\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Extraer JSON del texto generado\u001b[39;00m\n\u001b[32m     30\u001b[39m json_obj = extract_json_from_text(raw_output)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "texto_ejemplo_1 = \"\"\"\n",
    "Hola, necesito hacer un pedido urgente.\n",
    "\n",
    "Mi nombre es María García y mi correo es maria.garcia@example.com\n",
    "Mi teléfono es +34-555-123-456\n",
    "\n",
    "Quiero comprar:\n",
    "- 2 teclados inalámbricos\n",
    "- 1 mouse ergonómico\n",
    "\n",
    "Mi dirección es: Calle Mayor 123, Madrid, España, código postal 28013\n",
    "\n",
    "Necesito envío express para antes del 15 de diciembre de 2025.\n",
    "\n",
    "Gracias!\n",
    "\"\"\"\n",
    "\n",
    "resultado_1 = generar_orden_compra(texto_ejemplo_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 2: Pedido con Información Incompleta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo 3: Tu Propia Consulta\n",
    "\n",
    "**Escribe aquí tu propio texto para generar una orden de compra:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " TEXTO DE ENTRADA:\n",
      "================================================================================\n",
      "\n",
      "Escribe aquí tu orden de compra en lenguaje natural...\n",
      "\n",
      "\n",
      " Generando JSON...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ✏️ Modifica el texto a continuación con tu propia orden de compra\u001b[39;00m\n\u001b[32m      2\u001b[39m mi_texto = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mEscribe aquí tu orden de compra en lenguaje natural...\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m mi_resultado = generar_orden_compra(mi_texto)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mgenerar_orden_compra\u001b[39m\u001b[34m(texto_natural, verbose)\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Generando JSON...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Generar el JSON usando el modelo\u001b[39;00m\n\u001b[32m     20\u001b[39m raw_output = generate_json_raw(\n\u001b[32m     21\u001b[39m     text=texto_natural,\n\u001b[32m     22\u001b[39m     max_new_tokens=GEN_MAX_NEW_TOKENS,\n\u001b[32m     23\u001b[39m     max_length=MAX_LENGTH,\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     tokenizer=tokenizer,\n\u001b[32m     25\u001b[39m     model=model,\n\u001b[32m     26\u001b[39m     device=DEVICE\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Extraer JSON del texto generado\u001b[39;00m\n\u001b[32m     30\u001b[39m json_obj = extract_json_from_text(raw_output)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#  Modifica el texto a continuación con tu propia orden de compra\n",
    "mi_texto = \"\"\"\n",
    "Escribe aquí tu orden de compra en lenguaje natural...\n",
    "\"\"\"\n",
    "\n",
    "mi_resultado = generar_orden_compra(mi_texto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta Interactiva\n",
    "\n",
    "Usa esta celda para probar múltiples consultas de forma rápida:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribe tu consulta aquí y ejecuta la celda\n",
    "consulta = input(\"Escribe tu orden de compra en lenguaje natural:\\n\")\n",
    "resultado = generar_orden_compra(consulta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar Resultado\n",
    "\n",
    "Si quieres guardar algún resultado específico:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el último resultado generado\n",
    "output_file = \"mi_orden_compra.json\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(resultado, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Resultado guardado en: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_ejemplo_2 = \"\"\"\n",
    "Hola, quiero ordenar 5 laptops para mi oficina.\n",
    "Puedes contactarme al correo: juan.perez@company.com\n",
    "Prefiero recogerlo yo mismo en la tienda.\n",
    "\"\"\"\n",
    "\n",
    "resultado_2 = generar_orden_compra(texto_ejemplo_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new_entorno_2)",
   "language": "python",
   "name": "new_entorno_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
