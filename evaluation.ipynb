{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2d3188",
   "metadata": {
    "papermill": {
     "duration": 0,
     "end_time": "2025-11-29T02:44:20.357442",
     "exception": false,
     "start_time": "2025-11-29T02:44:20.357442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![Universidad_de_los_Andes_30.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd10e70",
   "metadata": {
    "papermill": {
     "duration": 0,
     "end_time": "2025-11-29T02:44:20.357442",
     "exception": false,
     "start_time": "2025-11-29T02:44:20.357442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "***\n",
    "# **Competencia: Modelos Avanzados PLN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b877b13-fa9f-4ee7-967a-02dff2433f92",
   "metadata": {},
   "source": [
    "***\n",
    "## Integrantes Equipo 20\n",
    "- Andr√©s Felipe √ëungo Fern√°ndez\n",
    "- Andr√©s Juli√°n Gonzalez Barrera\n",
    "- Hernando Jose Jimenez D√≠az\n",
    "- Gloria In√©s L√≥pez Urbano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c367f-ec4a-4488-b607-a020a18d8328",
   "metadata": {},
   "source": [
    "***\n",
    "# √çndice\n",
    "\n",
    "El *notebook* aborda la competencia con la siguiente estructura:\n",
    "\n",
    "| üîπ | Secci√≥n        |\n",
    "|----|----------------|\n",
    "| 1Ô∏è.   | **Instalaci√≥n e importe de librer√≠as** |\n",
    "...\n",
    "| 2Ô∏è.   | **Carga de datos y construcci√≥n prompt**  |\n",
    "...\n",
    "| 3Ô∏è. | **Desarrollo del modelo** |\n",
    "...\n",
    "| 4Ô∏è. | **Etapa Entrenamiento**   |\n",
    "...\n",
    "| 5Ô∏è. | **Cargue de los datos de validaci√≥n**  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b311f30d-6df1-442c-8fff-0102bf39ef8e",
   "metadata": {},
   "source": [
    "***\n",
    "# 1. Instalaci√≥n e importe de librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3041fd89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T02:44:20.365843Z",
     "iopub.status.busy": "2025-11-29T02:44:20.365843Z",
     "iopub.status.idle": "2025-11-29T02:44:27.179946Z",
     "shell.execute_reply": "2025-11-29T02:44:27.179946Z"
    },
    "papermill": {
     "duration": 6.814455,
     "end_time": "2025-11-29T02:44:27.179946",
     "exception": false,
     "start_time": "2025-11-29T02:44:20.365491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "hf_logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a777bf-e451-4761-9bff-8963f6b6aeeb",
   "metadata": {},
   "source": [
    "## 1.1 Definici√≥n variables globales y funciones auxiliares\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1db08",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Par√°metros principales\n",
    "\n",
    "MAX_LENGTH = 1500\n",
    "GEN_MAX_NEW_TOKENS = 377\n",
    "BATCH_EVAL = 48\n",
    "\n",
    "MAX_EVAL_EXAMPLES = None  # None para usar todo eval.json\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B-Base\"\n",
    "\n",
    "# Debe coincidir con train.py\n",
    "OUTPUT_DIR = \"output/results/v09\"\n",
    "MODEL_DIR = os.path.join(OUTPUT_DIR, \"modfinal\")\n",
    "\n",
    "DATA_PATH = \"data/eval/eval.json\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "GLB_SEED = 13\n",
    "\n",
    "def seed_everything(seed: int = 42, device: str = \"cpu\") -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "seed_everything(GLB_SEED, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eddfba-51ba-4c66-95c2-4da811f5adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones migradas \n",
    "\n",
    "def print_time_execution(description, start, end):\n",
    "    \"\"\"Devuelve un string con la duraci√≥n formateada.\"\"\"\n",
    "    str_log = \"\"\n",
    "    duration = end - start\n",
    "    if duration > 3600:\n",
    "        horas = duration // 3600\n",
    "        minutos = (duration % 3600) // 60\n",
    "        segundos = duration % 60\n",
    "        str_log = (\n",
    "            f\"Tiempo {description}: {horas:.0f} horas, \"\n",
    "            f\"{minutos:.0f} minutos y {segundos:.2f} segundos\"\n",
    "        )\n",
    "    elif duration > 60:\n",
    "        minutos = duration // 60\n",
    "        segundos = duration % 60\n",
    "        str_log = f\"Tiempo {description}: {minutos:.0f} minutos y {segundos:.2f} segundos\"\n",
    "    else:\n",
    "        str_log = f\"Tiempo {description}: {duration:.6f} segundos\"\n",
    "    return str_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b336e2-fee7-4144-ba5f-32d1867ec302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_memory():\n",
    "    \"Limpia VRAM agresivamente\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506155d2-5b59-4c35-abe1-fab414aba5e3",
   "metadata": {},
   "source": [
    "***\n",
    "# 2. Carga y construcci√≥n prompt\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7612f-2a42-4b49-8175-223ae18a3d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construcci√≥n del prompt \n",
    "def build_prompt(natural_text: str) -> str:\n",
    "    instructions = (\n",
    "        \"Eres un extractor de √≥rdenes de compra. Genera SOLO un JSON v√°lido \"\n",
    "        \"EXACTAMENTE con los campos requeridos.\\n\"\n",
    "        \"Reglas:\\n\"\n",
    "        \"- Usa null cuando un campo no exista.\\n\"\n",
    "        '- \\\"buyer\\\" debe existir; si name/email/contact/addresses faltan, d√©jalos en null.\\n'\n",
    "        '- Si addresses est√° vac√≠o o no existe -> \\\"addresses\\\": null.\\n'\n",
    "        '- Si purchases est√° vac√≠o o no existe -> \\\"purchases\\\": null.\\n'\n",
    "        '- shipping es opcional; si falta -> \\\"shipping\\\": null.\\n'\n",
    "        \"- Asegura que los tipos de datos principales sean correctos \"\n",
    "        \"(quantity: entero; country uno de US/CA/GB/ES/CO/DE/FR).\\n\"\n",
    "        \"- Usa null cuando no tengas informaci√≥n; NO inventes correos, tel√©fonos \"\n",
    "        \"ni c√≥digos de descuento si no aparecen.\\n\"\n",
    "        \"- Respeta exactamente los nombres de los campos del esquema.\\n\"\n",
    "        \"- Estructura el problema paso a paso, razona por etapas.\\n\\n\"\n",
    "    )\n",
    "    prompt = (\n",
    "        instructions\n",
    "        + \"Texto:\\n\"\n",
    "        + natural_text\n",
    "        + \"\\n\\nRESPONDE SOLO CON EL JSON V√ÅLIDO (nada m√°s):\\nJSON:\\n\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce10465-1003-4f00-924e-77e2535d66b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_text(text: str):\n",
    "    \"\"\"\n",
    "    Busca el primer '{' y el √∫ltimo '}' y trata de cargarlo como JSON.\n",
    "    Devuelve:\n",
    "      - dict/list si se pudo parsear\n",
    "      - None si no se pudo\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "\n",
    "    if start == -1 or end == -1 or end <= start:\n",
    "        return None\n",
    "\n",
    "    candidate = text[start : end + 1]\n",
    "\n",
    "    try:\n",
    "        return json.loads(candidate)\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac51ec45-b797-4e42-8fde-e4a8e82c67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_raw_batch(\n",
    "    texts,\n",
    "    max_new_tokens: int,\n",
    "    max_length: int,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    device: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Genera salidas del modelo para una lista de textos en lenguaje natural.\n",
    "    1. Construye el prompt con build_prompt.\n",
    "    2. Tokeniza con padding/truncation.\n",
    "    3. Llama a model.generate en modo batch.\n",
    "    4. Recorta la salida al primer bloque '{...}' si existe.\n",
    "\n",
    "    Devuelve: lista de strings (uno por texto original).\n",
    "    \"\"\"\n",
    "    # Construir prompts\n",
    "    prompts = [build_prompt(t) for t in texts]\n",
    "\n",
    "    enc = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Tokens especiales\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    eos_brace_id = tokenizer(\"}\", add_special_tokens=False).input_ids[0]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # determinista para evaluaci√≥n\n",
    "            pad_token_id=pad_id,\n",
    "            eos_token_id=eos_brace_id,  # cortar idealmente al cierre del JSON\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "\n",
    "    cleaned = []\n",
    "    for d in decoded:\n",
    "        if \"{\" in d and \"}\" in d:\n",
    "            first = d.find(\"{\")\n",
    "            last = d.rfind(\"}\")\n",
    "            cleaned.append(d[first : last + 1])\n",
    "        else:\n",
    "            cleaned.append(d)\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee10b05-1249-494e-ba55-24fc70c1ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de modelo y tokenizer\n",
    "def load_model_and_tokenizer():\n",
    "    print(\"Cargando tokenizer y modelo desde:\", MODEL_DIR)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    dtype = (\n",
    "        torch.bfloat16\n",
    "        if (torch.cuda.is_available() and torch.cuda.is_bf16_supported())\n",
    "        else torch.float16\n",
    "    )\n",
    "\n",
    "    # Carga del modelo QLoRA ya entrenado por train.py\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        MODEL_DIR,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"cuda\" if DEVICE == \"cuda\" else \"cpu\",\n",
    "        use_cache=True,\n",
    "    ).eval()\n",
    "\n",
    "    # Compilaci√≥n opcional\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3ceb2-eb94-4240-b2f5-e43d38b93ef1",
   "metadata": {},
   "source": [
    "***\n",
    "# 3. Inferencia\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f58e1-bd64-4dbe-a7d8-e207ab2c88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inferencia por batches\n",
    "def run_inference(dataset, tokenizer, model):\n",
    "    all_raw_outputs = []\n",
    "    n_total = len(dataset)\n",
    "\n",
    "    print(f\"\\nGenerando {n_total} predicciones...\")\n",
    "    for i in tqdm(range(0, n_total, BATCH_EVAL), desc=\"Inferencia\"):\n",
    "        batch_slice = dataset[i : i + BATCH_EVAL]\n",
    "        batch_texts = batch_slice[\"natural_language\"]\n",
    "\n",
    "        raw_batch = generate_json_raw_batch(\n",
    "            texts=batch_texts,\n",
    "            max_new_tokens=GEN_MAX_NEW_TOKENS,\n",
    "            max_length=MAX_LENGTH,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        all_raw_outputs.extend(raw_batch)\n",
    "\n",
    "    return all_raw_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdb26a-c290-41a1-89ca-7430bcf438d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline principal\n",
    "def main():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    total_start = time.time() \n",
    "    print(\"Cargando dataset de evaluaci√≥n y modelo...\")\n",
    "    load_start = time.time()   # <-- INICIO carga modelo/tokenizer\n",
    "    tokenizer, model = load_model_and_tokenizer()\n",
    "    load_end = time.time()     # <-- FIN carga modelo/tokenizer\n",
    "    print(print_time_execution(\"carga de modelo y tokenizer\", load_start, load_end))\n",
    "    \n",
    "    dataset = load_dataset(\"json\", data_files=DATA_PATH)[\"train\"]\n",
    "\n",
    "    # Si quieres probar con un subconjunto\n",
    "    if MAX_EVAL_EXAMPLES is not None:\n",
    "        max_n = min(MAX_EVAL_EXAMPLES, len(dataset))\n",
    "        dataset = dataset.select(range(max_n))\n",
    "\n",
    "    ids = dataset[\"id\"]\n",
    "\n",
    "    # 1) Inferencia por batches\n",
    "    inf_start = time.time()    # <-- INICIO inferencia\n",
    "    raw_outputs = run_inference(dataset, tokenizer, model)\n",
    "    inf_end = time.time()      # <-- FIN inferencia\n",
    "    print(print_time_execution(\"inferencia sobre eval.json\", inf_start, inf_end))\n",
    "\n",
    "    # Descargar modelo de VRAM\n",
    "    del model\n",
    "    clean_memory()\n",
    "\n",
    "    # 2) Procesar y guardar\n",
    "    print(\"\\n>>> Procesando resultados y construyendo JSON final...\")\n",
    "    final_predictions = []\n",
    "\n",
    "    for rec_id, raw in zip(ids, raw_outputs):\n",
    "        parsed = extract_json_from_text(raw)\n",
    "\n",
    "        if isinstance(parsed, dict):\n",
    "            final_json = parsed\n",
    "        else:\n",
    "            # En caso de fallo, entrega un JSON vac√≠o (o lo que defina la competencia)\n",
    "            final_json = {}\n",
    "\n",
    "        final_predictions.append(\n",
    "            {\n",
    "                \"id\": rec_id,\n",
    "                \"prediction\": json.dumps(final_json, ensure_ascii=False),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(final_predictions)\n",
    "\n",
    "    if MAX_EVAL_EXAMPLES is not None:\n",
    "        csv_path = os.path.join(OUTPUT_DIR, f\"submission{MAX_EVAL_EXAMPLES}.csv\")\n",
    "    else:\n",
    "        csv_path = os.path.join(OUTPUT_DIR, \"submission.csv\")\n",
    "\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"¬°Listo! Archivo guardado en: {csv_path}\")\n",
    "\n",
    "    total_end = time.time()  # <-- FIN tiempo total\n",
    "    print(print_time_execution(\"evaluaci√≥n completa (carga + inferencia + guardado)\", total_start, total_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da4d3a-1200-43b8-a4b0-742e48221acc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    clean_memory()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dbe801-885a-4638-b6c4-5fec36eaa804",
   "metadata": {},
   "source": [
    "# Celda para estilo de los markdowns (ignorar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "480b874f-78f1-4ecd-b767-360bc2ef3e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- Fuentes recomendadas para estilo matem√°tico -->\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=STIX+Two+Text:ital,wght@0,400;0,700;1,400;1,700&display=swap\" rel=\"stylesheet\">\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=STIX+Two+Math:ital,wght@0,400;0,700;1,400;1,700&display=swap\" rel=\"stylesheet\">\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap\" rel=\"stylesheet\">\n",
       "\n",
       "<style>\n",
       "  /* --- Markdown cl√°sico (Notebook) --- */\n",
       "  div.text_cell_render.rendered_html p,\n",
       "  div.text_cell_render.rendered_html h1,\n",
       "  div.text_cell_render.rendered_html h2,\n",
       "  div.text_cell_render.rendered_html h3,\n",
       "  div.text_cell_render.rendered_html li {\n",
       "    font-family: 'STIX Two Text', serif !important;\n",
       "  }\n",
       "\n",
       "  /* MathJax (f√≥rmulas) */\n",
       "  .MathJax, .MathJax_Display {\n",
       "    font-family: 'STIX Two Math', serif !important;\n",
       "  }\n",
       "\n",
       "  /* C√≥digo */\n",
       "  div.text_cell_render.rendered_html code,\n",
       "  div.text_cell_render.rendered_html pre {\n",
       "    font-family: 'Inconsolata', monospace !important;\n",
       "  }\n",
       "\n",
       "  /* --- JupyterLab --- */\n",
       "  .jp-Notebook .jp-RenderedHTMLCommon p,\n",
       "  .jp-Notebook .jp-RenderedHTMLCommon h1,\n",
       "  .jp-Notebook .jp-RenderedHTMLCommon h2,\n",
       "  .jp-Notebook .jp-RenderedHTMLCommon h3,\n",
       "  .jp-Notebook .jp-RenderedHTMLCommon li {\n",
       "    font-family: 'STIX Two Text', serif !important;\n",
       "  }\n",
       "  .jp-Notebook .jp-RenderedHTMLCommon code,\n",
       "  .jp-Notebook .jp-RenderedHTMLCommon pre {\n",
       "    font-family: 'Inconsolata', monospace !important;\n",
       "  }\n",
       "  .jp-Notebook .MathJax,\n",
       "  .jp-Notebook .MathJax_Display {\n",
       "    font-family: 'STIX Two Math', serif !important;\n",
       "  }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<!-- Fuentes recomendadas para estilo matem√°tico -->\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=STIX+Two+Text:ital,wght@0,400;0,700;1,400;1,700&display=swap\" rel=\"stylesheet\">\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=STIX+Two+Math:ital,wght@0,400;0,700;1,400;1,700&display=swap\" rel=\"stylesheet\">\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;700&display=swap\" rel=\"stylesheet\">\n",
    "\n",
    "<style>\n",
    "  /* --- Markdown cl√°sico (Notebook) --- */\n",
    "  div.text_cell_render.rendered_html p,\n",
    "  div.text_cell_render.rendered_html h1,\n",
    "  div.text_cell_render.rendered_html h2,\n",
    "  div.text_cell_render.rendered_html h3,\n",
    "  div.text_cell_render.rendered_html li {\n",
    "    font-family: 'STIX Two Text', serif !important;\n",
    "  }\n",
    "\n",
    "  /* MathJax (f√≥rmulas) */\n",
    "  .MathJax, .MathJax_Display {\n",
    "    font-family: 'STIX Two Math', serif !important;\n",
    "  }\n",
    "\n",
    "  /* C√≥digo */\n",
    "  div.text_cell_render.rendered_html code,\n",
    "  div.text_cell_render.rendered_html pre {\n",
    "    font-family: 'Inconsolata', monospace !important;\n",
    "  }\n",
    "\n",
    "  /* --- JupyterLab --- */\n",
    "  .jp-Notebook .jp-RenderedHTMLCommon p,\n",
    "  .jp-Notebook .jp-RenderedHTMLCommon h1,\n",
    "  .jp-Notebook .jp-RenderedHTMLCommon h2,\n",
    "  .jp-Notebook .jp-RenderedHTMLCommon h3,\n",
    "  .jp-Notebook .jp-RenderedHTMLCommon li {\n",
    "    font-family: 'STIX Two Text', serif !important;\n",
    "  }\n",
    "  .jp-Notebook .jp-RenderedHTMLCommon code,\n",
    "  .jp-Notebook .jp-RenderedHTMLCommon pre {\n",
    "    font-family: 'Inconsolata', monospace !important;\n",
    "  }\n",
    "  .jp-Notebook .MathJax,\n",
    "  .jp-Notebook .MathJax_Display {\n",
    "    font-family: 'STIX Two Math', serif !important;\n",
    "  }\n",
    "</style>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.999054,
   "end_time": "2025-11-29T02:44:28.079332",
   "environment_variables": {},
   "exception": null,
   "input_path": "evaluation.ipynb",
   "output_path": "evaluation_output.ipynb",
   "parameters": {},
   "start_time": "2025-11-29T02:44:19.080278",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
