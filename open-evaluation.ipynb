{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Universidad_de_los_Andes_30.png](data:image/png;base64,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)\n",
    "\n",
    "# Open Evaluation - Consultas Abiertas en Lenguaje Natural\n",
    "\n",
    "## DescripciÃ³n del Proyecto\n",
    "\n",
    "Este notebook demuestra la capacidad del modelo entrenado para procesar **consultas abiertas** en lenguaje natural y generar Ã³rdenes de compra en formato JSON estructurado.\n",
    "\n",
    "**Objetivo**: Validar que el modelo puede generalizar a textos no vistos durante el entrenamiento y generar JSONs vÃ¡lidos con estructura consistente segÃºn el esquema de la competencia.\n",
    "\n",
    "**Contexto de negocio**: Una compaÃ±Ã­a de ventas recibe pedidos por mÃºltiples canales (email, SMS, Markdown). Este modelo automatiza la conversiÃ³n a un formato estÃ¡ndar JSON para procesamiento posterior.\n",
    "\n",
    "---\n",
    "\n",
    "## Integrantes Equipo 20\n",
    "- AndrÃ©s Felipe Ã‘ungo FernÃ¡ndez  \n",
    "- AndrÃ©s JuliÃ¡n Gonzalez Barrera  \n",
    "- Hernando Jose Jimenez DÃ­az  \n",
    "- Gloria InÃ©s LÃ³pez Urbano\n",
    "\n",
    "---\n",
    "\n",
    "## Requisitos del Proyecto\n",
    "\n",
    "Este notebook es uno de los **4 entregables obligatorios** del proyecto final:\n",
    "\n",
    "| # | Archivo | DescripciÃ³n |\n",
    "|---|---------|-------------|\n",
    "| 1 | `train.ipynb` | Entrenamiento del modelo ||\n",
    "| 2 | `weights.pt` | Pesos entrenados (generado por train)  |\n",
    "| 3 | `evaluation.ipynb` | EvaluaciÃ³n en test set |  |\n",
    "| 4 | **`open-evaluation.ipynb`** | **Este notebook** - Consultas abiertas  |\n",
    "\n",
    "---\n",
    "\n",
    "## Estructura del Notebook\n",
    "\n",
    "1. **ConfiguraciÃ³n Inicial**: LibrerÃ­as, dispositivo, y parÃ¡metros\n",
    "2. **Carga del Modelo**: Tokenizer + Modelo base + Adaptadores LoRA\n",
    "3. **FunciÃ³n Principal**: `generar_orden_compra()`\n",
    "4. **Ejemplos de Uso**: 4+ casos variados (bÃ¡sico, completo, incompleto, multiproducto)\n",
    "5. **Consulta Interactiva**: Para probar tus propios textos\n",
    "6. **AnÃ¡lisis de Resultados**: VerificaciÃ³n de estructura JSON\n",
    "7. **ExportaciÃ³n**: Guardar resultados\n",
    "\n",
    "---\n",
    "\n",
    "## Esquema JSON Esperado\n",
    "\n",
    "El modelo genera JSONs con la siguiente estructura:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"buyer\": {\n",
    "    \"name\": \"string\",\n",
    "    \"email\": \"string\",\n",
    "    \"contact\": {\"phone\": \"string\", \"preferred_contact\": \"email|phone|none\"},\n",
    "    \"addresses\": [{\"street\": \"...\", \"city\": \"...\", \"country\": \"...\"}]\n",
    "  },\n",
    "  \"purchases\": [\n",
    "    {\"product_name\": \"string\", \"quantity\": integer, \"currency\": \"USD|EUR|GBP\"}\n",
    "  ],\n",
    "  \"shipping\": {\n",
    "    \"method\": \"standard|express|pickup\",\n",
    "    \"preferred_by\": \"datetime\"\n",
    "  }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. ConfiguraciÃ³n Inicial\n",
    "\n",
    "Esta secciÃ³n prepara el entorno de ejecuciÃ³n:\n",
    "- **LibrerÃ­as**: torch, transformers, PEFT para inferencia\n",
    "- **Dispositivo**: DetecciÃ³n automÃ¡tica de CUDA/CPU\n",
    "- **Semillas**: Reproducibilidad de resultados\n",
    "- **ParÃ¡metros**: ConfiguraciÃ³n del modelo y generaciÃ³n\n",
    "\n",
    "**Nota**: Este notebook requiere que `train.ipynb` haya sido ejecutado previamente para generar el modelo entrenado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Importar LibrerÃ­as\n",
    "\n",
    "**LibrerÃ­as principales:**\n",
    "- `torch`: Framework de deep learning\n",
    "- `transformers`: Modelos pre-entrenados (Qwen3-0.6B)\n",
    "- `peft`: Parameter-Efficient Fine-Tuning (LoRA)\n",
    "- `shared_functions`: Funciones personalizadas para construcciÃ³n de prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftModel\n",
    "import shared_functions as custom_sharfun  # Funciones para construir prompts\n",
    "\n",
    "# ConfiguraciÃ³n de warnings\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_warning()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 ConfiguraciÃ³n del Dispositivo y Semillas\n",
    "\n",
    "**ConfiguraciÃ³n de reproducibilidad:**\n",
    "- Semilla fija (42) para resultados consistentes\n",
    "- DetecciÃ³n automÃ¡tica de GPU (CUDA) o CPU\n",
    "- ConfiguraciÃ³n de semillas para torch, random y numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfiguraciÃ³n del dispositivo\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "GLB_SEED = 42\n",
    "torch.manual_seed(GLB_SEED)\n",
    "random.seed(GLB_SEED)\n",
    "np.random.seed(GLB_SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(GLB_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 ParÃ¡metros de ConfiguraciÃ³n\n",
    "\n",
    "**ParÃ¡metros del modelo:**\n",
    "- `MAX_LENGTH`: 1500 tokens (longitud mÃ¡xima de entrada)\n",
    "- `GEN_MAX_NEW_TOKENS`: 377 tokens (tokens mÃ¡ximos a generar)\n",
    "- `MODEL_DIR`: Directorio con el modelo entrenado\n",
    "\n",
    "**Batch size adaptativo:**\n",
    "- GPU: 14 ejemplos por batch\n",
    "- CPU: 8 ejemplos por batch (reduce memoria pero es mÃ¡s lento)\n",
    "\n",
    "** Importante**: Verifica que el directorio del modelo exista antes de continuar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: ConfiguraciÃ³n adaptada para modfinal_v04\n",
    "# ------------------------------------------------------------------\n",
    "# EvaluaciÃ³n\n",
    "BATCH_SIZE_EVAL = 14 if DEVICE == \"cuda\" else 8  # Ajustado para CPU/GPU\n",
    "\n",
    "# Data / tokenization\n",
    "MAX_LENGTH = 1500\n",
    "GEN_MAX_NEW_TOKENS = 377\n",
    "\n",
    "# ðŸ”§ MODELO: Usar modfinal_v04 (tu modelo entrenado)\n",
    "MODEL_DIR = \"modfinal_v04\"\n",
    "ADAPTER_DIR = MODEL_DIR\n",
    "OUTPUT_DIR = \"output/open_evaluation\"  # Para guardar resultados de open evaluation\n",
    "\n",
    "print(f\" Modelo a usar: {ADAPTER_DIR}\")\n",
    "print(f\" Verificando si existe...\")\n",
    "if os.path.exists(ADAPTER_DIR):\n",
    "    print(f\" Modelo encontrado en: {os.path.abspath(ADAPTER_DIR)}\")\n",
    "else:\n",
    "    print(f\" ERROR: No se encuentra {ADAPTER_DIR}\")\n",
    "    print(f\"   Directorios disponibles:\")\n",
    "    for item in os.listdir(\".\"):\n",
    "        if os.path.isdir(item) and \"mod\" in item.lower():\n",
    "            print(f\"   - {item}\")\n",
    "\n",
    "# Crear directorio de salida si no existe\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\" Directorio de salida: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Carga del Modelo Entrenado\n",
    "\n",
    "## 2.1 Proceso de Carga\n",
    "\n",
    "El modelo se carga en 3 pasos:\n",
    "1. **Tokenizer**: ConfiguraciÃ³n con `padding_side=\"left\"` (importante para generaciÃ³n)\n",
    "2. **Modelo base**: Qwen3-0.6B con o sin cuantizaciÃ³n segÃºn dispositivo\n",
    "3. **Adaptadores LoRA**: Pesos entrenados aplicados al modelo base\n",
    "\n",
    "**ConfiguraciÃ³n segÃºn dispositivo:**\n",
    "- **GPU (CUDA)**: Modelo cuantizado 4-bit â†’ Reduce memoria ~60% con mÃ­nima pÃ©rdida de calidad\n",
    "- **CPU**: Modelo en float32 â†’ Sin cuantizaciÃ³n (mÃ¡s lento pero funcional)\n",
    "\n",
    "**Modo evaluaciÃ³n**: `model.eval()` desactiva dropout y pone BatchNorm en modo inferencia.\n",
    "\n",
    "** Nota**: Respetar `tokenizer.padding_side = \"left\"` es crÃ­tico para que la generaciÃ³n funcione correctamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Cargando tokenizer desde: {ADAPTER_DIR}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR, local_files_only=True)\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B-Base\"\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\" Cargando modelo base en 4 bits (GPU)...\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "else:\n",
    "    print(\"  CPU detectado - Cargando modelo SIN cuantizaciÃ³n (puede ser lento)...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float32,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    base_model = base_model.to(DEVICE)\n",
    "\n",
    "print(f\" Cargando adaptadores LoRA desde: {ADAPTER_DIR}\")\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_DIR, local_files_only=True)\n",
    "model.eval()\n",
    "print(\" Modelo cargado exitosamente en modo evaluaciÃ³n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Open Evaluation - Consultas Abiertas\n",
    "\n",
    "## 3.1 Â¿QuÃ© es Open Evaluation?\n",
    "\n",
    "**Open Evaluation** permite probar el modelo con **textos completamente nuevos** que nunca vio durante el entrenamiento.\n",
    "\n",
    "**Diferencias con evaluaciÃ³n estÃ¡ndar:**\n",
    "- **EvaluaciÃ³n estÃ¡ndar**: Dataset fijo con respuestas conocidas â†’ Mide F1 score\n",
    "- **Open Evaluation**: Textos libres sin respuestas conocidas â†’ Verifica generalizaciÃ³n\n",
    "\n",
    "**Casos de uso:**\n",
    "- âœ… Probar pedidos reales de clientes\n",
    "- âœ… Evaluar robustez ante textos mal escritos o incompletos\n",
    "- âœ… Demostrar capacidades del modelo a stakeholders\n",
    "- âœ… Detectar fallos en casos edge (datos faltantes, formatos raros)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 FunciÃ³n Principal: `generar_orden_compra()`\n",
    "\n",
    "Esta funciÃ³n es el punto de entrada para consultas abiertas:\n",
    "\n",
    "**Entrada:**\n",
    "- `texto_natural` (str): Pedido en lenguaje natural (email, SMS, etc.)\n",
    "- `verbose` (bool): Si es True, muestra el JSON generado formateado\n",
    "\n",
    "**Proceso interno:**\n",
    "1. **ConstrucciÃ³n del prompt**: Usa `custom_sharfun.build_prompt_text_to_json()`\n",
    "2. **TokenizaciÃ³n**: Convierte texto â†’ tokens (mÃ¡x 1500)\n",
    "3. **GeneraciÃ³n**: Modelo produce tokens nuevos (mÃ¡x 377)\n",
    "4. **ExtracciÃ³n**: Busca y parsea el JSON en la respuesta\n",
    "5. **ValidaciÃ³n**: Verifica que sea JSON vÃ¡lido\n",
    "\n",
    "**Salida:**\n",
    "- `dict`: JSON parseado si exitoso\n",
    "- `None`: Si no se pudo generar/parsear JSON\n",
    "\n",
    "** Importante**: Esta funciÃ³n NO calcula F1 score porque no tiene \"ground truth\" para comparar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_orden_compra(texto_natural: str, verbose: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Genera una orden de compra en formato JSON a partir de texto en lenguaje natural.\n",
    "    \n",
    "    Args:\n",
    "        texto_natural: Texto en lenguaje natural describiendo la orden de compra\n",
    "        verbose: Si True, imprime informaciÃ³n adicional del proceso\n",
    "    \n",
    "    Returns:\n",
    "        dict: JSON con la orden de compra estructurada\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" TEXTO DE ENTRADA:\")\n",
    "        print(\"=\"*80)\n",
    "        print(texto_natural)\n",
    "        print(\"\\n Generando JSON...\\n\")\n",
    "    \n",
    "    # Generar el JSON usando el modelo\n",
    "    raw_output = generate_json_raw(\n",
    "        text=texto_natural,\n",
    "        max_new_tokens=GEN_MAX_NEW_TOKENS,\n",
    "        max_length=MAX_LENGTH,\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # Extraer JSON del texto generado\n",
    "    json_obj = extract_json_from_text(raw_output)\n",
    "    \n",
    "    if json_obj is None:\n",
    "        if verbose:\n",
    "            print(\" No se pudo extraer un JSON vÃ¡lido\")\n",
    "            print(\"\\n Salida cruda del modelo:\")\n",
    "            print(raw_output)\n",
    "        return {}\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*80)\n",
    "        print(\" JSON GENERADO:\")\n",
    "        print(\"=\"*80)\n",
    "        print(json.dumps(json_obj, indent=2, ensure_ascii=False))\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    return json_obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Ejemplos de Uso\n",
    "\n",
    "### Ejemplo 1: Pedido Completo y Bien Estructurado\n",
    "\n",
    "**Caso:** Cliente proporciona toda la informaciÃ³n necesaria de forma clara.\n",
    "\n",
    "**Expectativa:** El modelo debe extraer correctamente:\n",
    "- Nombre y contacto del comprador\n",
    "- Lista de productos con cantidades\n",
    "- DirecciÃ³n de envÃ­o completa\n",
    "- MÃ©todo de envÃ­o y fecha preferida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_ejemplo_1 = \"\"\"\n",
    "Hola, necesito hacer un pedido urgente.\n",
    "\n",
    "Mi nombre es MarÃ­a GarcÃ­a y mi correo es maria.garcia@example.com\n",
    "Mi telÃ©fono es +34-555-123-456\n",
    "\n",
    "Quiero comprar:\n",
    "- 2 teclados inalÃ¡mbricos\n",
    "- 1 mouse ergonÃ³mico\n",
    "\n",
    "Mi direcciÃ³n es: Calle Mayor 123, Madrid, EspaÃ±a, cÃ³digo postal 28013\n",
    "\n",
    "Necesito envÃ­o express para antes del 15 de diciembre de 2025.\n",
    "\n",
    "Gracias!\n",
    "\"\"\"\n",
    "\n",
    "resultado_1 = generar_orden_compra(texto_ejemplo_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2: Pedido con InformaciÃ³n Incompleta\n",
    "\n",
    "**Caso:** Cliente omite algunos detalles (sin email, sin telÃ©fono, sin fecha).\n",
    "\n",
    "**Expectativa:** El modelo debe:\n",
    "- Extraer lo que estÃ¡ disponible\n",
    "- Usar valores por defecto razonables para lo faltante\n",
    "- Mantener la estructura JSON vÃ¡lida\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 3: Tu Propia Consulta\n",
    "\n",
    "**Instrucciones:**\n",
    "1. Modifica el texto en la celda siguiente\n",
    "2. Ejecuta la celda para ver el JSON generado\n",
    "3. Observa cÃ³mo el modelo interpreta tu texto\n",
    "\n",
    "** Tips para mejores resultados:**\n",
    "- Incluye nombre del comprador\n",
    "- Especifica productos y cantidades claramente\n",
    "- Menciona direcciÃ³n de envÃ­o\n",
    "- Indica mÃ©todo de envÃ­o si es importante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Modifica el texto a continuaciÃ³n con tu propia orden de compra\n",
    "mi_texto = \"\"\"\n",
    "Escribe aquÃ­ tu orden de compra en lenguaje natural...\n",
    "\"\"\"\n",
    "\n",
    "mi_resultado = generar_orden_compra(mi_texto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.4 Consulta Interactiva\n",
    "\n",
    "**Modo interactivo**: Ejecuta esta celda para ingresar tu propia consulta desde el teclado.\n",
    "\n",
    "**Casos de prueba sugeridos:**\n",
    "- Pedido en lenguaje informal (como SMS)\n",
    "- Pedido con errores de ortografÃ­a\n",
    "- Pedido con mÃºltiples productos\n",
    "- Pedido internacional (diferentes paÃ­ses)\n",
    "- Pedido urgente vs. envÃ­o estÃ¡ndar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribe tu consulta aquÃ­ y ejecuta la celda\n",
    "consulta = input(\"Escribe tu orden de compra en lenguaje natural:\\n\")\n",
    "resultado = generar_orden_compra(consulta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.5 Guardar Resultados\n",
    "\n",
    "**ExportaciÃ³n de resultados**: Si necesitas guardar el JSON generado para su posterior uso.\n",
    "\n",
    "**Formato de salida:**\n",
    "- Archivo `.json` con indentaciÃ³n legible\n",
    "- CodificaciÃ³n UTF-8 (soporta caracteres especiales)\n",
    "- Nombre personalizable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el Ãºltimo resultado generado\n",
    "output_file = \"mi_orden_compra.json\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(resultado, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Resultado guardado en: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Troubleshooting y Tips\n",
    "\n",
    "### Problemas Comunes\n",
    "\n",
    "**1. El modelo genera texto pero no JSON vÃ¡lido**\n",
    "- **Causa**: Prompt poco claro o texto de entrada muy ambiguo\n",
    "- **SoluciÃ³n**: Agrega mÃ¡s detalles (nombre, productos, direcciÃ³n)\n",
    "\n",
    "**2. JSON incompleto o con campos vacÃ­os**\n",
    "- **Causa**: InformaciÃ³n faltante en el texto de entrada\n",
    "- **SoluciÃ³n**: El modelo usa valores por defecto; verifica si son aceptables\n",
    "\n",
    "**3. GeneraciÃ³n muy lenta (CPU)**\n",
    "- **Causa**: CPU es ~10x mÃ¡s lento que GPU\n",
    "- **SoluciÃ³n**: Normal; considera usar GPU o reducir `MAX_LENGTH`\n",
    "\n",
    "**4. Error \"CUDA out of memory\"**\n",
    "- **Causa**: GPU sin memoria suficiente\n",
    "- **SoluciÃ³n**: Reinicia el kernel o reduce `BATCH_SIZE`\n",
    "\n",
    "### Tips de Mejores PrÃ¡cticas\n",
    "\n",
    " **Para entregas del proyecto:**\n",
    "- Incluye al menos 4 ejemplos variados\n",
    "- Muestra casos exitosos Y casos con informaciÃ³n incompleta\n",
    "- Comenta los resultados obtenidos\n",
    "\n",
    " **Para uso real:**\n",
    "- Valida el JSON generado antes de usarlo\n",
    "- Ten un fallback para casos donde el modelo falla\n",
    "- Loguea consultas que fallan para mejorar el modelo\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š Resumen del Notebook\n",
    "\n",
    "###  Lo que completamos\n",
    "\n",
    "1. **ConfiguraciÃ³n**: Cargamos el modelo entrenado con LoRA\n",
    "2. **FunciÃ³n principal**: `generar_orden_compra()` lista para usar\n",
    "3. **Ejemplos**: Probamos 3+ casos de uso diferentes\n",
    "4. **Interactividad**: Modo para probar tus propias consultas\n",
    "5. **ExportaciÃ³n**: Guardar resultados en JSON\n",
    "\n",
    "###  PrÃ³ximos Pasos\n",
    "\n",
    "Para completar el proyecto, asegÃºrate de tener:\n",
    "- [] `train.ipynb` - Entrenamiento completado\n",
    "- [] `weights.pt` - Modelo guardado\n",
    "- [] `evaluation.ipynb` - EvaluaciÃ³n en test set con F1 score\n",
    "- [] **`open-evaluation.ipynb`** - Este notebook completado\n",
    "\n",
    "###  Referencias\n",
    "\n",
    "**Documentos del proyecto:**\n",
    "- `Enunciado_competencia_MANLP_2025_02.pdf`\n",
    "- `evaluation_metric.py`\n",
    "- `shared_functions.py`\n",
    "\n",
    "**Modelos y datos:**\n",
    "- Modelo base: `Qwen/Qwen3-0.6B-Base`\n",
    "- Adaptadores: `modfinal_v04/`\n",
    "- Dataset: `data/train/` y `data/eval/`\n",
    "\n",
    "---\n",
    "\n",
    "**Â¡Notebook completado exitosamente!**\n",
    "\n",
    "*Este notebook demuestra la capacidad del modelo para generalizar a textos no vistos durante el entrenamiento.*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generar_orden_compra' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m texto_ejemplo_2 = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33mHola, quiero ordenar 5 laptops para mi oficina.\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mPuedes contactarme al correo: juan.perez@company.com\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mPrefiero recogerlo yo mismo en la tienda.\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m resultado_2 = generar_orden_compra(texto_ejemplo_2)\n",
      "\u001b[31mNameError\u001b[39m: name 'generar_orden_compra' is not defined"
     ]
    }
   ],
   "source": [
    "texto_ejemplo_2 = \"\"\"\n",
    "Hola, quiero ordenar 5 laptops para mi oficina.\n",
    "Puedes contactarme al correo: juan.perez@company.com\n",
    "Prefiero recogerlo yo mismo en la tienda.\n",
    "\"\"\"\n",
    "\n",
    "resultado_2 = generar_orden_compra(texto_ejemplo_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new_entorno_2)",
   "language": "python",
   "name": "new_entorno_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
