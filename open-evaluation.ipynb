{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Open Evaluation - Consulta Abierta en Lenguaje Natural\n",
        "\n",
        "Este notebook permite realizar consultas abiertas en lenguaje natural y obtener una respuesta JSON generada por el modelo entrenado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Librer√≠as\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import shared_functions as custom_sharfun\n",
        "\n",
        "from peft import PeftModel\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from transformers import logging as hf_logging\n",
        "hf_logging.set_verbosity_warning()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuraci√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPORTANTE: dejar las mismas configuraciones que train.ipynb\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "# Data / tokenization\n",
        "MAX_LENGTH = 1252  # Mismo valor que en train.ipynb\n",
        "\n",
        "# Evaluaci√≥n\n",
        "GEN_MAX_NEW_TOKENS = 377  # Mismo valor que en train.ipynb\n",
        "\n",
        "# Configuraci√≥n del dispositivo\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "GLB_SEED = 42\n",
        "torch.manual_seed(GLB_SEED)\n",
        "random.seed(GLB_SEED)\n",
        "if DEVICE == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(GLB_SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cargar Modelo Entrenado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"Qwen/Qwen3-0.6B-Base\"\n",
        "WEIGHTS_PATH = \"weights.pt\"  # Pesos en la ra√≠z del proyecto\n",
        "OUTPUT_DIR = \"output/results/v01\"\n",
        "ADAPTER_DIR = os.path.join(OUTPUT_DIR, \"modfinal\")\n",
        "\n",
        "print(\"Cargando tokenizer y modelo...\")\n",
        "\n",
        "# Cargar tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR, trust_remote_code=True)\n",
        "\n",
        "# Configuraci√≥n de cuantizaci√≥n\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Cargar modelo base\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Cargar adaptadores LoRA\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ Modelo cargado exitosamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funci√≥n de Consulta Abierta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generar_orden_compra(texto_natural: str, verbose: bool = True) -> dict:\n",
        "    \"\"\"\n",
        "    Genera una orden de compra en formato JSON a partir de texto en lenguaje natural.\n",
        "    \n",
        "    Args:\n",
        "        texto_natural: Texto en lenguaje natural describiendo la orden de compra\n",
        "        verbose: Si True, imprime informaci√≥n adicional del proceso\n",
        "    \n",
        "    Returns:\n",
        "        dict: JSON con la orden de compra estructurada\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üìù TEXTO DE ENTRADA:\")\n",
        "        print(\"=\"*80)\n",
        "        print(texto_natural)\n",
        "        print(\"\\nüîÑ Generando JSON...\\n\")\n",
        "    \n",
        "    # Generar el JSON usando el modelo\n",
        "    raw_output = custom_sharfun.generate_json_raw(\n",
        "        text=texto_natural,\n",
        "        max_new_tokens=GEN_MAX_NEW_TOKENS,\n",
        "        max_length=MAX_LENGTH,\n",
        "        tokenizer=tokenizer,\n",
        "        model=model,\n",
        "        device=DEVICE\n",
        "    )\n",
        "    \n",
        "    # Extraer JSON del texto generado\n",
        "    json_obj = custom_sharfun.extract_json_from_text(raw_output)\n",
        "    \n",
        "    if json_obj is None:\n",
        "        if verbose:\n",
        "            print(\"‚ö†Ô∏è  No se pudo extraer un JSON v√°lido\")\n",
        "            print(\"\\nüìÑ Salida cruda del modelo:\")\n",
        "            print(raw_output)\n",
        "        return {}\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"=\"*80)\n",
        "        print(\"‚úÖ JSON GENERADO:\")\n",
        "        print(\"=\"*80)\n",
        "        print(json.dumps(json_obj, indent=2, ensure_ascii=False))\n",
        "        print(\"=\"*80)\n",
        "    \n",
        "    return json_obj\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejemplos de Uso\n",
        "\n",
        "### Ejemplo 1: Pedido Simple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texto_ejemplo_1 = \"\"\"\n",
        "Hola, necesito hacer un pedido urgente.\n",
        "\n",
        "Mi nombre es Mar√≠a Garc√≠a y mi correo es maria.garcia@example.com\n",
        "Mi tel√©fono es +34-555-123-456\n",
        "\n",
        "Quiero comprar:\n",
        "- 2 teclados inal√°mbricos\n",
        "- 1 mouse ergon√≥mico\n",
        "\n",
        "Mi direcci√≥n es: Calle Mayor 123, Madrid, Espa√±a, c√≥digo postal 28013\n",
        "\n",
        "Necesito env√≠o express para antes del 15 de diciembre de 2025.\n",
        "\n",
        "Gracias!\n",
        "\"\"\"\n",
        "\n",
        "resultado_1 = generar_orden_compra(texto_ejemplo_1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejemplo 2: Pedido con Informaci√≥n Incompleta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texto_ejemplo_2 = \"\"\"\n",
        "Hola, quiero ordenar 5 laptops para mi oficina.\n",
        "Puedes contactarme al correo: juan.perez@company.com\n",
        "Prefiero recogerlo yo mismo en la tienda.\n",
        "\"\"\"\n",
        "\n",
        "resultado_2 = generar_orden_compra(texto_ejemplo_2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejemplo 3: Tu Propia Consulta\n",
        "\n",
        "**Escribe aqu√≠ tu propio texto para generar una orden de compra:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚úèÔ∏è Modifica el texto a continuaci√≥n con tu propia orden de compra\n",
        "mi_texto = \"\"\"\n",
        "Escribe aqu√≠ tu orden de compra en lenguaje natural...\n",
        "\"\"\"\n",
        "\n",
        "mi_resultado = generar_orden_compra(mi_texto)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Consulta Interactiva\n",
        "\n",
        "Usa esta celda para probar m√∫ltiples consultas de forma r√°pida:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Escribe tu consulta aqu√≠ y ejecuta la celda\n",
        "consulta = input(\"Escribe tu orden de compra en lenguaje natural:\\n\")\n",
        "resultado = generar_orden_compra(consulta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Guardar Resultado\n",
        "\n",
        "Si quieres guardar alg√∫n resultado espec√≠fico:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar el √∫ltimo resultado generado\n",
        "output_file = \"mi_orden_compra.json\"\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(resultado, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úÖ Resultado guardado en: {output_file}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
