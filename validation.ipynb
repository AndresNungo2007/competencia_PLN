{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2d3188",
   "metadata": {},
   "source": [
    "# Revisar el score del 10% de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd10e70",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from typing import List\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shared_functions as custom_sharfun  #el archivo .py con funciones compartidas\n",
    "import evaluation_metric as custom_metrics\n",
    "\n",
    "from peft import PeftModel, LoraConfig\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4fc226",
   "metadata": {},
   "source": [
    "# Cargar el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eeaac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios y modelos\n",
    "OUTPUT_DIR = \"output/results/v10\"\n",
    "ADAPTER_DIR = os.path.join(OUTPUT_DIR, \"modfinal_full\")\n",
    "\n",
    "# Configuración del dispositivo\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4e9334",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_file = os.path.join(ADAPTER_DIR, \"weights.pt\")\n",
    "pt_loaded = torch.load(pt_file, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "print(\"Cargando tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pt_loaded[\"tokenizer\"])\n",
    "tokenizer.padding_side = \"left\" # A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Cargando modelo base en 4 bits...\")\n",
    "bnb_config = BitsAndBytesConfig(**pt_loaded[\"bnb_config\"])\n",
    "\n",
    "print(\"Modelo base:\", pt_loaded[\"model_id\"])\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    pt_loaded[\"model_id\"],\n",
    "    quantization_config=bnb_config, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Cargando modelo LoRA...\")\n",
    "# Debes usar la misma configuración LoRA que usaste al entrenar\n",
    "config_lora = LoraConfig(\n",
    "    r = pt_loaded[\"config\"][\"lora_r\"],\n",
    "    lora_alpha = pt_loaded[\"config\"][\"lora_alpha\"],\n",
    "    lora_dropout = pt_loaded[\"config\"][\"lora_drop\"],\n",
    "    target_modules = pt_loaded[\"config\"][\"lora_target_mods\"],\n",
    "    bias=pt_loaded[\"config\"][\"lora_bias\"],\n",
    "    task_type=pt_loaded[\"config\"][\"lora_task_type\"]\n",
    ")\n",
    "\n",
    "model = PeftModel(model_base, config_lora)\n",
    "model.load_state_dict(pt_loaded[\"peft\"], strict=False)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf608e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data / tokenization\n",
    "MAX_LENGTH = 2048\n",
    "# Evaluación\n",
    "GEN_MAX_NEW_TOKENS = 512\n",
    "BATCH_SIZE_EVAL = 14 # ajustar según memoria GPU para ejecutar más rápido la evaluación\n",
    "\n",
    "# Semilla de entrenamiento\n",
    "GLB_SEED = pt_loaded[\"config\"][\"seed\"]\n",
    "torch.manual_seed(GLB_SEED)\n",
    "random.seed(GLB_SEED)\n",
    "np.random.seed(GLB_SEED)\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(GLB_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ade697",
   "metadata": {},
   "source": [
    "# Cargando los datos de validación\n",
    "Del 10% de los datos que fueron dividos en la etapa de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4117bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR_VALDATA = os.path.join(OUTPUT_DIR, \"datavalidation\")\n",
    "val_list = load_from_disk(OUTPUT_DIR_VALDATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4256d7",
   "metadata": {},
   "source": [
    "## Funciones adaptadas para ejecutar en lotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed1d817",
   "metadata": {},
   "source": [
    "padding=\"longest\",       # respeta left-padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b11c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_json_raw_batch( texts: List[str], tokenizer, model, device, max_new_tokens: int, max_length: int, batch_size: int = 8):\n",
    "    outputs = []\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    #eos_brace_id = tokenizer.encode(\"}\", add_special_tokens=False)[0]\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating\", total=math.ceil(len(texts)/batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        prompts = [custom_sharfun.build_prompt(t) for t in batch]\n",
    "\n",
    "        enc = tokenizer( prompts, return_tensors='pt', truncation=True, padding=\"longest\", max_length=max_length).to(device)\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        attention_mask = enc[\"attention_mask\"]\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            model.eval()\n",
    "            out = model.generate( \n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                max_new_tokens=max_new_tokens, \n",
    "                do_sample=False, \n",
    "                pad_token_id=pad_id, \n",
    "                eos_token_id=tokenizer.eos_token_id, \n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.batch_decode(out, skip_special_tokens=True) # Decodificar outputs en lote\n",
    "\n",
    "        # Recorte final\n",
    "        cleaned = []\n",
    "        for d in decoded:\n",
    "            d = (d.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\"))\n",
    "\n",
    "            if \"{\" in d and \"}\" in d:\n",
    "                first = d.find(\"{\")\n",
    "                last = d.rfind(\"}\")\n",
    "                d = d[first:last+1]\n",
    "            cleaned.append(d)\n",
    "\n",
    "        outputs.extend(cleaned)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e66126",
   "metadata": {},
   "source": [
    "Funcion mejorada para extraer el JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c677d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_text(text: str):\n",
    "    \"\"\"\n",
    "    Escanea llaves para encontrar un bloque JSON bien balanceado.\n",
    "    \"\"\"\n",
    "    marker = '{\"buyer\":'\n",
    "    pos = text.find(marker)\n",
    "    if pos != -1:\n",
    "        start = text.find(marker)\n",
    "    else: # pos == -1\n",
    "        marker = \"\\nJSON:\\n\"\n",
    "        pos = text.find(marker)\n",
    "        if pos == -1:\n",
    "            start = text.find(\"{\")\n",
    "        else:\n",
    "            start = text.find(\"{\", pos + len(marker)) # Buscar la primera llave '{' después del marcador\n",
    "            if start == -1:\n",
    "                start = text.find(\"{\")\n",
    "\n",
    "    if start == -1:\n",
    "        return None\n",
    "\n",
    "    brace_count = 0\n",
    "    in_json = False\n",
    "\n",
    "    for i in range(start, len(text)):\n",
    "        if text[i] == \"{\":\n",
    "            brace_count += 1\n",
    "            in_json = True\n",
    "        elif text[i] == \"}\":\n",
    "            brace_count -= 1\n",
    "\n",
    "            # Si brace_count llega a 0 => JSON completo\n",
    "            if in_json and brace_count == 0:\n",
    "                candidate = text[start:i+1]\n",
    "\n",
    "                # intentar parsear\n",
    "                try:\n",
    "                    return json.loads(candidate)\n",
    "                except json.JSONDecodeError:\n",
    "                    # intento reemplazando comillas simples\n",
    "                    try:\n",
    "                        return json.loads(candidate.replace(\"'\", '\"'))\n",
    "                    except Exception:\n",
    "                        return None\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print(f\"Generando validación para evaluar F1-score: total datos = {len(val_list)} ...\")\n",
    "\n",
    "texts = [ex[\"natural_language\"] for ex in val_list]\n",
    "true_jsons = [ex[\"json_data\"] for ex in val_list]\n",
    "nat_langs = texts\n",
    "\n",
    "pred_raw_list = generate_json_raw_batch( texts=texts, tokenizer=tokenizer, model=model, device=DEVICE, max_new_tokens=GEN_MAX_NEW_TOKENS,  max_length=MAX_LENGTH, batch_size=BATCH_SIZE_EVAL )\n",
    "results = []\n",
    "for nat_langs_save, raw, true_json in zip(nat_langs, pred_raw_list, true_jsons):\n",
    "    pred_obj = extract_json_from_text(raw)\n",
    "    if pred_obj is None:\n",
    "        pred_obj = {}\n",
    "    \n",
    "    f1 = 0.0\n",
    "    if pred_obj is None:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        try:\n",
    "            f1 = custom_metrics.evaluate_json(true_json, json.dumps(pred_obj, ensure_ascii=False))\n",
    "        except Exception:\n",
    "            f1 = float(1.0 if pred_obj == true_json else 0.0)\n",
    "\n",
    "    results.append({\n",
    "        \"nat_language\": nat_langs_save,\n",
    "        \"raw_prediction\": raw,\n",
    "        \"prediction\": pred_obj,\n",
    "        \"true_json\": true_json,\n",
    "        \"f1_score\": f1\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "print( custom_sharfun.print_time_execution(\"Etapa validación de datos\", start_time, end_time) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Resumen de resultados en test: ver histrograma de F1 scores\n",
    "f1_scores = [r['f1_score'] for r in results]\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, len(f1_scores) + 1), f1_scores, marker='o')\n",
    "plt.title(\"F1 Scores por Ejemplo de Validación\")\n",
    "plt.xlabel(\"Ejemplo de Validación\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e23c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar CSV - resultados de validación\n",
    "OUTPUT_DIR_VAL = os.path.join(OUTPUT_DIR, \"result_validation\")\n",
    "os.makedirs(OUTPUT_DIR_VAL, exist_ok=True)\n",
    "csv_path = os.path.join(OUTPUT_DIR_VAL, 'validation_results.csv')\n",
    "with open(csv_path, 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['text','f1','raw','pred','true'], delimiter='|')\n",
    "    writer.writeheader()\n",
    "    for r in results:\n",
    "        writer.writerow({\n",
    "            'text': r['nat_language'],\n",
    "            'f1': r['f1_score'],\n",
    "            'raw': r['raw_prediction'],\n",
    "            'pred': json.dumps(r['prediction'], ensure_ascii=False),\n",
    "            'true': json.dumps(r['true_json'], ensure_ascii=False)\n",
    "        })\n",
    "print('CSV guardado en', csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d273c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma F1\n",
    "f1_scores = [r['f1_score'] for r in results]\n",
    "plt.figure()\n",
    "plt.hist(f1_scores, bins=10)\n",
    "plt.title('Distribución de F1')\n",
    "plt.xlabel('F1')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'f1_distribution.png'))\n",
    "plt.show()\n",
    "plt.close()\n",
    "print('Histograma guardado en', os.path.join(OUTPUT_DIR, 'f1_distribution.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d7735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar peores 3 ejemplos\n",
    "sorted_by_f1 = sorted(results, key=lambda x: x['f1_score'])\n",
    "print('\\nPeores 10 ejemplos:')\n",
    "for r in sorted_by_f1[:10]:\n",
    "    print(f\"F1 Score: {r['f1_score']}\")\n",
    "    print('Texto:', r['raw'])\n",
    "    print(\"*\"*90)\n",
    "    print('Pred:', r['pred'])\n",
    "    print('True:', r['true'])\n",
    "    print('-'*150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_312_CUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
